{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 50 Homework Assignments - Python & Pandas for Machine Learning\n",
        "\n",
        "## ðŸ“š Overview\n",
        "\n",
        "This notebook contains **50 comprehensive homework assignments** covering:\n",
        "- **Part 1**: Python Data Structures (Tuples, Sets, Dictionaries) - 15 problems\n",
        "- **Part 2**: Python Functions & Functional Programming - 10 problems\n",
        "- **Part 3**: Pandas Basics & Data Manipulation - 15 problems\n",
        "- **Part 4**: Pandas Advanced Operations - 10 problems\n",
        "\n",
        "### ðŸ“Š Difficulty Levels:\n",
        "- ðŸŸ¢ **Easy**: Basic concepts, direct application\n",
        "- ðŸŸ¡ **Medium**: Multiple concepts, some logic required\n",
        "- ðŸ”´ **Hard**: Complex logic, multiple steps, real-world scenarios\n",
        "\n",
        "### ðŸ“ Instructions:\n",
        "1. Read each problem carefully\n",
        "2. Write your solution in the provided code cell\n",
        "3. Test your solution with the given examples\n",
        "4. Compare with the solution (hidden in collapsed cells)\n",
        "5. Try to solve without looking at solutions first!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Python Data Structures (15 Problems)\n",
        "\n",
        "## Section A: Tuples (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 1: Image Dimensions Handler ðŸŸ¢\n",
        "\n",
        "**Topic**: Tuple unpacking and operations\n",
        "\n",
        "**Description**:\n",
        "You're working with image data in machine learning. Create a function that:\n",
        "1. Takes an image dimension tuple `(height, width, channels)`\n",
        "2. Returns a dictionary with keys: 'height', 'width', 'channels', 'total_pixels', 'is_grayscale'\n",
        "3. `total_pixels = height Ã— width`\n",
        "4. `is_grayscale = True` if channels == 1, else False\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "image_info((224, 224, 3))\n",
        "# Output: {'height': 224, 'width': 224, 'channels': 3, 'total_pixels': 50176, 'is_grayscale': False}\n",
        "\n",
        "image_info((128, 128, 1))\n",
        "# Output: {'height': 128, 'width': 128, 'channels': 1, 'total_pixels': 16384, 'is_grayscale': True}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'height': 224, 'width': 224, 'channels': 3, 'total_pixels': 50176, 'is_grayscale': False}\n",
            "{'height': 128, 'width': 128, 'channels': 1, 'total_pixels': 16384, 'is_grayscale': True}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def image_info(dimensions):\n",
        "    \"\"\"\n",
        "    Extracts image information from a dimensions tuple.\n",
        "    \n",
        "    Args:\n",
        "        dimensions (tuple): A tuple containing (height, width, channels)\n",
        "        \n",
        "    Returns:\n",
        "        dict: A dictionary with image metadata\n",
        "    \"\"\"\n",
        "    # Unpack the tuple into variables for clarity\n",
        "    height, width, channels = dimensions\n",
        "    \n",
        "    # Calculate the total number of pixels\n",
        "    total_pixels = height * width\n",
        "    \n",
        "    # Determine if the image is grayscale (1 channel)\n",
        "    is_grayscale = (channels == 1)\n",
        "    \n",
        "    # Return the dictionary with all required keys\n",
        "    return {\n",
        "        'height': height,\n",
        "        'width': width,\n",
        "        'channels': channels,\n",
        "        'total_pixels': total_pixels,\n",
        "        'is_grayscale': is_grayscale\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: dimensions = (100, 100, 3)\n",
        "# 1. height=100, width=100, channels=3\n",
        "# 2. total_pixels = 100 * 100 = 10000\n",
        "# 3. is_grayscale = (3 == 1) -> False\n",
        "# 4. Result: {'height': 100, 'width': 100, 'channels': 3, 'total_pixels': 10000, 'is_grayscale': False}\n",
        "\n",
        "# Test your solution\n",
        "print(image_info((224, 224, 3)))\n",
        "print(image_info((128, 128, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2: Coordinate Distance Calculator ðŸŸ¢\n",
        "\n",
        "**Topic**: Tuple arithmetic and math operations\n",
        "\n",
        "**Description**:\n",
        "Calculate the Euclidean distance between two points in 2D space.\n",
        "- Formula: `distance = âˆš((x2-x1)Â² + (y2-y1)Â²)`\n",
        "- Input: Two tuples `(x, y)` representing coordinates\n",
        "- Output: Float (rounded to 2 decimal places)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "distance((0, 0), (3, 4))  # Output: 5.0\n",
        "distance((1, 2), (4, 6))  # Output: 5.0\n",
        "```\n",
        "\n",
        "**Hint**: Use `math.sqrt()` or `** 0.5` for square root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.0\n",
            "5.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import math\n",
        "\n",
        "def distance(point1, point2):\n",
        "    \"\"\"\n",
        "    Calculates the Euclidean distance between two 2D points.\n",
        "    \n",
        "    Args:\n",
        "        point1 (tuple): (x, y) coordinates of the first point\n",
        "        point2 (tuple): (x, y) coordinates of the second point\n",
        "        \n",
        "    Returns:\n",
        "        float: The distance rounded to 2 decimal places\n",
        "    \"\"\"\n",
        "    # Unpack coordinates\n",
        "    x1, y1 = point1\n",
        "    x2, y2 = point2\n",
        "    \n",
        "    # Apply Euclidean distance formula: sqrt((x2-x1)^2 + (y2-y1)^2)\n",
        "    dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "    \n",
        "    # Return rounded result\n",
        "    return round(dist, 2)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: point1=(0,0), point2=(3,4)\n",
        "# 1. x1=0, y1=0, x2=3, y2=4\n",
        "# 2. (x2-x1)^2 = (3-0)^2 = 9\n",
        "# 3. (y2-y1)^2 = (4-0)^2 = 16\n",
        "# 4. sqrt(9 + 16) = sqrt(25) = 5.0\n",
        "# 5. Round(5.0, 2) -> 5.0\n",
        "\n",
        "# Test your solution\n",
        "print(distance((0, 0), (3, 4)))\n",
        "print(distance((1, 2), (4, 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3: Training Configuration Validator ðŸŸ¡\n",
        "\n",
        "**Topic**: Tuple immutability and validation\n",
        "\n",
        "**Description**:\n",
        "Create a function that validates ML training configurations stored as tuples:\n",
        "- Input: `(learning_rate, batch_size, epochs, optimizer)`\n",
        "- Validate:\n",
        "  - `learning_rate`: must be between 0.0001 and 1.0\n",
        "  - `batch_size`: must be power of 2 (16, 32, 64, 128, etc.)\n",
        "  - `epochs`: must be positive integer\n",
        "  - `optimizer`: must be one of ['adam', 'sgd', 'rmsprop']\n",
        "- Return: tuple `(is_valid, error_messages_list)`\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "validate_config((0.001, 32, 100, 'adam'))\n",
        "# Output: (True, [])\n",
        "\n",
        "validate_config((2.0, 30, -5, 'bad_optimizer'))\n",
        "# Output: (False, ['learning_rate out of range', 'batch_size not power of 2', 'epochs must be positive', 'invalid optimizer'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(True, [])\n",
            "(False, ['learning_rate out of range', 'batch_size not power of 2', 'epochs must be positive', 'invalid optimizer'])\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def validate_config(config):\n",
        "    \"\"\"\n",
        "    Validates a machine learning training configuration tuple.\n",
        "    \n",
        "    Args:\n",
        "        config (tuple): (learning_rate, batch_size, epochs, optimizer)\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (is_valid, list_of_error_messages)\n",
        "    \"\"\"\n",
        "    lr, batch, epochs, optimizer = config\n",
        "    errors = []\n",
        "    \n",
        "    # Check learning rate range\n",
        "    if not (0.0001 <= lr <= 1.0):\n",
        "        errors.append('learning_rate out of range')\n",
        "    \n",
        "    # Check batch size is power of 2 using bitwise check\n",
        "    # (batch & (batch-1) == 0) is true only for powers of 2\n",
        "    if not (batch > 0 and (batch & (batch - 1) == 0)):\n",
        "        errors.append('batch_size not power of 2')\n",
        "        \n",
        "    # Check epochs is positive\n",
        "    if epochs <= 0:\n",
        "        errors.append('epochs must be positive')\n",
        "        \n",
        "    # Check valid optimizer\n",
        "    if optimizer not in ['adam', 'sgd', 'rmsprop']:\n",
        "        errors.append('invalid optimizer')\n",
        "        \n",
        "    return len(errors) == 0, errors\n",
        "\n",
        "# Dry Run:\n",
        "# Input: config = (2.0, 30, -5, 'bad')\n",
        "# 1. lr=2.0 (Invalid > 1.0) -> Add error\n",
        "# 2. batch=30 (Not power of 2: 30 & 29 != 0) -> Add error\n",
        "# 3. epochs=-5 (<= 0) -> Add error\n",
        "# 4. optimizer='bad' (Not in list) -> Add error\n",
        "# 5. len(errors) > 0 -> Returns (False, ['learning_rate...', ...])\n",
        "\n",
        "# Test your solution\n",
        "print(validate_config((0.001, 32, 100, 'adam')))\n",
        "print(validate_config((2.0, 30, -5, 'bad_optimizer')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 4: Data Split Generator ðŸŸ¡\n",
        "\n",
        "**Topic**: Tuple creation and list manipulation\n",
        "\n",
        "**Description**:\n",
        "Create a function that splits data indices for train/validation/test sets:\n",
        "- Input: `total_samples` (int), `train_ratio`, `val_ratio`, `test_ratio` (floats that sum to 1.0)\n",
        "- Output: Tuple of three ranges: `(train_indices, val_indices, test_indices)`\n",
        "- Each element should be a tuple of (start_index, end_index)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "split_data(100, 0.7, 0.2, 0.1)\n",
        "# Output: ((0, 70), (70, 90), (90, 100))\n",
        "\n",
        "split_data(1000, 0.8, 0.1, 0.1)\n",
        "# Output: ((0, 800), (800, 900), (900, 1000))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((0, 70), (70, 90), (90, 100))\n",
            "((0, 800), (800, 900), (900, 1000))\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def split_data(total_samples, train_ratio, val_ratio, test_ratio):\n",
        "    \"\"\"\n",
        "    Generates index ranges for data splitting.\n",
        "    \n",
        "    Args:\n",
        "        total_samples (int): Total number of samples\n",
        "        train_ratio (float): Ratio of training data\n",
        "        val_ratio (float): Ratio of validation data\n",
        "        test_ratio (float): Ratio of testing data\n",
        "        \n",
        "    Returns:\n",
        "        tuple: ((train_start, train_end), (val_start, val_end), (test_start, test_end))\n",
        "    \"\"\"\n",
        "    # Calculate end index for training set\n",
        "    train_end = int(total_samples * train_ratio)\n",
        "    \n",
        "    # Calculate end index for validation set (starts where train ends)\n",
        "    val_end = train_end + int(total_samples * val_ratio)\n",
        "    \n",
        "    # Test set goes from validation end to total samples\n",
        "    # We use total_samples as constant to ensure we use all data even with rounding\n",
        "    \n",
        "    return (\n",
        "        (0, train_end),\n",
        "        (train_end, val_end),\n",
        "        (val_end, total_samples)\n",
        "    )\n",
        "\n",
        "# Dry Run:\n",
        "# Input: total=100, train=0.7, val=0.2, test=0.1\n",
        "# 1. train_end = int(100 * 0.7) = 70\n",
        "# 2. val_end = 70 + int(100 * 0.2) = 70 + 20 = 90\n",
        "# 3. Result: ((0, 70), (70, 90), (90, 100))\n",
        "\n",
        "# Test your solution\n",
        "print(split_data(100, 0.7, 0.2, 0.1))\n",
        "print(split_data(1000, 0.8, 0.1, 0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 5: Model Metrics Comparison ðŸ”´\n",
        "\n",
        "**Topic**: Complex tuple operations and sorting\n",
        "\n",
        "**Description**:\n",
        "You have multiple ML models with their performance metrics as tuples:\n",
        "- Input: List of tuples `[(model_name, accuracy, precision, recall, f1_score), ...]`\n",
        "- Tasks:\n",
        "  1. Find the best model for each metric\n",
        "  2. Calculate average of all metrics across models\n",
        "  3. Return a dictionary with: `'best_models'`, `'averages'`, `'ranked_by_f1'`\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "models = [\n",
        "    ('Model_A', 0.85, 0.82, 0.88, 0.85),\n",
        "    ('Model_B', 0.92, 0.90, 0.85, 0.87),\n",
        "    ('Model_C', 0.88, 0.85, 0.92, 0.88)\n",
        "]\n",
        "\n",
        "compare_models(models)\n",
        "# Output: {\n",
        "#     'best_models': {\n",
        "#         'accuracy': 'Model_B',\n",
        "#         'precision': 'Model_B',\n",
        "#         'recall': 'Model_C',\n",
        "#         'f1_score': 'Model_C'\n",
        "#     },\n",
        "#     'averages': {'accuracy': 0.88, 'precision': 0.86, 'recall': 0.88, 'f1_score': 0.87},\n",
        "#     'ranked_by_f1': ['Model_C', 'Model_B', 'Model_A']\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'best_models': {'accuracy': 'Model_B', 'precision': 'Model_B', 'recall': 'Model_C', 'f1_score': 'Model_C'}, 'averages': {'accuracy': 0.88, 'precision': 0.86, 'recall': 0.88, 'f1_score': 0.87}, 'ranked_by_f1': ['Model_C', 'Model_B', 'Model_A']}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def compare_models(models):\n",
        "    \"\"\"\n",
        "    Compares multiple ML models and identifies the best performers.\n",
        "    \n",
        "    Args:\n",
        "        models (list): List of tuples (name, accuracy, precision, recall, f1)\n",
        "        \n",
        "    Returns:\n",
        "        dict: Analysis results with best models and averages\n",
        "    \"\"\"\n",
        "    names = [m[0] for m in models]\n",
        "    accuracies = [m[1] for m in models]\n",
        "    precisions = [m[2] for m in models]\n",
        "    recalls = [m[3] for m in models]\n",
        "    f1_scores = [m[4] for m in models]\n",
        "    \n",
        "    metrics_map = {\n",
        "        'accuracy': accuracies, \n",
        "        'precision': precisions,\n",
        "        'recall': recalls,\n",
        "        'f1_score': f1_scores\n",
        "    }\n",
        "    \n",
        "    # Find best model for each metric\n",
        "    best_models = {}\n",
        "    for metric, values in metrics_map.items():\n",
        "        best_idx = values.index(max(values))\n",
        "        best_models[metric] = names[best_idx]\n",
        "        \n",
        "    # Calculate averages\n",
        "    averages = {k: round(sum(v)/len(v), 2) for k, v in metrics_map.items()}\n",
        "    \n",
        "    # Sort models by f1_score descending\n",
        "    # We use zip to pair rank with name, sort, and extract name\n",
        "    ranked_by_f1 = [\n",
        "        name for score, name in sorted(zip(f1_scores, names), reverse=True)\n",
        "    ]\n",
        "    \n",
        "    return {\n",
        "        'best_models': best_models,\n",
        "        'averages': averages,\n",
        "        'ranked_by_f1': ranked_by_f1\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: [(A, 0.8, ...), (B, 0.9, ...)]\n",
        "# 1. metrics_map['accuracy'] = [0.8, 0.9]\n",
        "# 2. best 'accuracy' -> max is 0.9 (index 1) -> Model B\n",
        "# 3. averages['accuracy'] -> (0.8+0.9)/2 = 0.85\n",
        "# 4. Sort by F1 -> List of names ordered by F1 desc\n",
        "\n",
        "# Test your solution\n",
        "models = [\n",
        "    ('Model_A', 0.85, 0.82, 0.88, 0.85),\n",
        "    ('Model_B', 0.92, 0.90, 0.85, 0.87),\n",
        "    ('Model_C', 0.88, 0.85, 0.92, 0.88)\n",
        "]\n",
        "print(compare_models(models))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section B: Sets (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 6: Duplicate Detector ðŸŸ¢\n",
        "\n",
        "**Topic**: Set basics and uniqueness\n",
        "\n",
        "**Description**:\n",
        "Find duplicate values in a list and return statistics:\n",
        "- Input: List of any values\n",
        "- Output: Dictionary with:\n",
        "  - `'has_duplicates'`: Boolean\n",
        "  - `'unique_count'`: Number of unique values\n",
        "  - `'duplicate_count'`: Number of duplicate values\n",
        "  - `'duplicates'`: Set of duplicate values\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "find_duplicates([1, 2, 3, 2, 4, 3, 5])\n",
        "# Output: {'has_duplicates': True, 'unique_count': 5, 'duplicate_count': 2, 'duplicates': {2, 3}}\n",
        "\n",
        "find_duplicates([1, 2, 3, 4, 5])\n",
        "# Output: {'has_duplicates': False, 'unique_count': 5, 'duplicate_count': 0, 'duplicates': set()}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'has_duplicates': True, 'unique_count': 5, 'duplicate_count': 2, 'duplicates': {2, 3}}\n",
            "{'has_duplicates': False, 'unique_count': 5, 'duplicate_count': 0, 'duplicates': set()}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def find_duplicates(data):\n",
        "    \"\"\"\n",
        "    Identifies duplicate values in a list.\n",
        "    \n",
        "    Args:\n",
        "        data (list): Input list of values\n",
        "        \n",
        "    Returns:\n",
        "        dict: Statistics about duplicates\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    duplicates = set()\n",
        "    \n",
        "    for item in data:\n",
        "        # If item already in 'seen', it's a duplicate\n",
        "        if item in seen:\n",
        "            duplicates.add(item)\n",
        "        else:\n",
        "            seen.add(item)\n",
        "            \n",
        "    return {\n",
        "        'has_duplicates': len(duplicates) > 0,\n",
        "        'unique_count': len(seen),\n",
        "        'duplicate_count': len(data) - len(seen),  # Total items - Unique items\n",
        "        'duplicates': duplicates\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: [1, 2, 2, 3]\n",
        "# 1. item=1: seen={1}\n",
        "# 2. item=2: seen={1, 2}\n",
        "# 3. item=2: in seen -> duplicates={2}\n",
        "# 4. item=3: seen={1, 2, 3}\n",
        "# Result: has_duplicates=True, unique_count=3, duplicate_count=1, duplicates={2}\n",
        "\n",
        "# Test your solution\n",
        "print(find_duplicates([1, 2, 3, 2, 4, 3, 5]))\n",
        "print(find_duplicates([1, 2, 3, 4, 5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 7: Feature Engineering - Categorical Encoder ðŸŸ¡\n",
        "\n",
        "**Topic**: Set operations for ML preprocessing\n",
        "\n",
        "**Description**:\n",
        "Create a simple categorical encoder:\n",
        "- Input: List of categorical values (strings)\n",
        "- Output: Dictionary with:\n",
        "  - `'encoding_map'`: Dictionary mapping each unique category to an integer (0, 1, 2, ...)\n",
        "  - `'encoded_data'`: List of encoded integers\n",
        "  - `'num_categories'`: Number of unique categories\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "encode_categories(['cat', 'dog', 'cat', 'bird', 'dog', 'cat'])\n",
        "# Output: {\n",
        "#     'encoding_map': {'cat': 0, 'dog': 1, 'bird': 2},\n",
        "#     'encoded_data': [0, 1, 0, 2, 1, 0],\n",
        "#     'num_categories': 3\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'encoding_map': {'bird': 0, 'cat': 1, 'dog': 2}, 'encoded_data': [1, 2, 1, 0, 2, 1], 'num_categories': 3}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def encode_categories(data):\n",
        "    \"\"\"\n",
        "    Performs label encoding on a list of categorical values.\n",
        "    \n",
        "    Args:\n",
        "        data (list): List of category strings\n",
        "        \n",
        "    Returns:\n",
        "        dict: Encoding map and encoded data\n",
        "    \"\"\"\n",
        "    # Find unique categories and sort them for consistent mapping\n",
        "    unique_cats = sorted(list(set(data)))\n",
        "    \n",
        "    # Create dictionary mapping category -> integer index\n",
        "    encoding_map = {cat: i for i, cat in enumerate(unique_cats)}\n",
        "    \n",
        "    # Transform the original data using the map\n",
        "    encoded_data = [encoding_map[cat] for cat in data]\n",
        "    \n",
        "    return {\n",
        "        'encoding_map': encoding_map,\n",
        "        'encoded_data': encoded_data,\n",
        "        'num_categories': len(unique_cats)\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: ['a', 'b', 'a']\n",
        "# 1. unique = ['a', 'b'] (sorted)\n",
        "# 2. map = {'a': 0, 'b': 1}\n",
        "# 3. encoded = [map['a'], map['b'], map['a']] = [0, 1, 0]\n",
        "\n",
        "# Test your solution\n",
        "print(encode_categories(['cat', 'dog', 'cat', 'bird', 'dog', 'cat']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 8: Dataset Overlap Analyzer ðŸŸ¡\n",
        "\n",
        "**Topic**: Set operations (union, intersection, difference)\n",
        "\n",
        "**Description**:\n",
        "Analyze overlap between training and test datasets:\n",
        "- Input: Two lists (train_ids, test_ids)\n",
        "- Output: Dictionary with:\n",
        "  - `'total_unique'`: Total unique IDs across both sets\n",
        "  - `'overlap'`: IDs present in both sets (data leakage!)\n",
        "  - `'only_train'`: IDs only in training\n",
        "  - `'only_test'`: IDs only in test\n",
        "  - `'has_leakage'`: Boolean (True if overlap exists)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "analyze_overlap([1, 2, 3, 4, 5], [4, 5, 6, 7, 8])\n",
        "# Output: {\n",
        "#     'total_unique': 8,\n",
        "#     'overlap': {4, 5},\n",
        "#     'only_train': {1, 2, 3},\n",
        "#     'only_test': {6, 7, 8},\n",
        "#     'has_leakage': True\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'total_unique': 8, 'overlap': {4, 5}, 'only_train': {1, 2, 3}, 'only_test': {8, 6, 7}, 'has_leakage': True}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def analyze_overlap(train_ids, test_ids):\n",
        "    \"\"\"\n",
        "    Analyzes data leakage between training and test sets.\n",
        "    \n",
        "    Args:\n",
        "        train_ids (list): List of IDs in training set\n",
        "        test_ids (list): List of IDs in test set\n",
        "        \n",
        "    Returns:\n",
        "        dict: Analysis of overlap\n",
        "    \"\"\"\n",
        "    train_set = set(train_ids)\n",
        "    test_set = set(test_ids)\n",
        "    \n",
        "    # Intersection finds elements present in BOTH sets\n",
        "    overlap = train_set.intersection(test_set)\n",
        "    \n",
        "    return {\n",
        "        'total_unique': len(train_set.union(test_set)),\n",
        "        'overlap': overlap,\n",
        "        'only_train': train_set - test_set,  # Difference\n",
        "        'only_test': test_set - train_set,\n",
        "        'has_leakage': len(overlap) > 0\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: train=[1, 2], test=[2, 3]\n",
        "# 1. sets: {1, 2}, {2, 3}\n",
        "# 2. overlap: {2}\n",
        "# 3. only_train: {1}, only_test: {3}\n",
        "# 4. leakage: True (since overlap size > 0)\n",
        "\n",
        "# Test your solution\n",
        "print(analyze_overlap([1, 2, 3, 4, 5], [4, 5, 6, 7, 8]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 9: Text Preprocessing - Stopword Remover ðŸŸ¡\n",
        "\n",
        "**Topic**: Set operations for text processing\n",
        "\n",
        "**Description**:\n",
        "Remove stopwords from text using sets:\n",
        "- Input: Text string and set of stopwords\n",
        "- Output: Dictionary with:\n",
        "  - `'original_words'`: List of all words\n",
        "  - `'filtered_words'`: List of words after removing stopwords\n",
        "  - `'removed_count'`: Number of words removed\n",
        "  - `'unique_filtered'`: Set of unique words after filtering\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "stopwords = {'the', 'is', 'a', 'an', 'in', 'on', 'at'}\n",
        "text = \"the cat is on the mat in the house\"\n",
        "\n",
        "remove_stopwords(text, stopwords)\n",
        "# Output: {\n",
        "#     'original_words': ['the', 'cat', 'is', 'on', 'the', 'mat', 'in', 'the', 'house'],\n",
        "#     'filtered_words': ['cat', 'mat', 'house'],\n",
        "#     'removed_count': 6,\n",
        "#     'unique_filtered': {'cat', 'mat', 'house'}\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'original_words': ['the', 'cat', 'is', 'on', 'the', 'mat', 'in', 'the', 'house'], 'filtered_words': ['cat', 'mat', 'house'], 'removed_count': 6, 'unique_filtered': {'cat', 'house', 'mat'}}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def remove_stopwords(text, stopwords):\n",
        "    \"\"\"\n",
        "    Removes common stopwords from a text string.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text\n",
        "        stopwords (set): Set of words to remove\n",
        "        \n",
        "    Returns:\n",
        "        dict: Filtered results\n",
        "    \"\"\"\n",
        "    original_words = text.split()\n",
        "    \n",
        "    # Filter words that are NOT in the stopwords set\n",
        "    filtered_words = [w for w in original_words if w not in stopwords]\n",
        "    \n",
        "    return {\n",
        "        'original_words': original_words,\n",
        "        'filtered_words': filtered_words,\n",
        "        'removed_count': len(original_words) - len(filtered_words),\n",
        "        'unique_filtered': set(filtered_words)\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: \"a cat\", stopwords={'a'}\n",
        "# 1. original=['a', 'cat']\n",
        "# 2. 'a' in stopwords -> remove\n",
        "# 3. 'cat' not in stopwords -> keep\n",
        "# 4. filtered=['cat']\n",
        "# 5. removed_count = 2 - 1 = 1\n",
        "\n",
        "# Test your solution\n",
        "stopwords = {'the', 'is', 'a', 'an', 'in', 'on', 'at'}\n",
        "text = \"the cat is on the mat in the house\"\n",
        "print(remove_stopwords(text, stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 10: Feature Selection - Correlation Filter ðŸ”´\n",
        "\n",
        "**Topic**: Advanced set operations\n",
        "\n",
        "**Description**:\n",
        "Filter highly correlated feature pairs:\n",
        "- Input: Dictionary of feature pairs and their correlation: `{('f1', 'f2'): 0.95, ...}`\n",
        "- Threshold: Correlation threshold (e.g., 0.9)\n",
        "- Output: Dictionary with:\n",
        "  - `'highly_correlated_pairs'`: Set of feature pairs above threshold\n",
        "  - `'features_to_remove'`: Set of features to remove (keep only one from each pair)\n",
        "  - `'features_to_keep'`: Set of features to keep\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "correlations = {\n",
        "    ('f1', 'f2'): 0.95,\n",
        "    ('f1', 'f3'): 0.5,\n",
        "    ('f2', 'f4'): 0.92,\n",
        "    ('f3', 'f4'): 0.3\n",
        "}\n",
        "\n",
        "filter_correlated(correlations, threshold=0.9)\n",
        "# Output: {\n",
        "#     'highly_correlated_pairs': {('f1', 'f2'), ('f2', 'f4')},\n",
        "#     'features_to_remove': {'f2'},  # f2 appears in both correlated pairs\n",
        "#     'features_to_keep': {'f1', 'f3', 'f4'}\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'highly_correlated_pairs': {('f2', 'f4'), ('f1', 'f2')}, 'features_to_remove': {'f4', 'f2'}, 'features_to_keep': {'f1', 'f3'}}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def filter_correlated(correlations, threshold):\n",
        "    \"\"\"\n",
        "    Identify features to remove based on high correlation.\n",
        "    \n",
        "    Args:\n",
        "        correlations (dict): {(feat1, feat2): correlation_value}\n",
        "        threshold (float): Cutoff for high correlation\n",
        "        \n",
        "    Returns:\n",
        "        dict: Features to keep and remove\n",
        "    \"\"\"\n",
        "    # Find all pairs above threshold\n",
        "    highly_correlated = {pair for pair, corr in correlations.items() if abs(corr) >= threshold}\n",
        "    \n",
        "    to_remove = set()\n",
        "    for (f1, f2) in highly_correlated:\n",
        "        # If neither feature is already marked for removal, remove the second one\n",
        "        # This is a greedy approach to break the correlation\n",
        "        if f1 not in to_remove and f2 not in to_remove:\n",
        "            to_remove.add(f2)\n",
        "            \n",
        "    # Get universe of all features mentioned\n",
        "    all_features = set()\n",
        "    for f1, f2 in correlations.keys():\n",
        "        all_features.add(f1)\n",
        "        all_features.add(f2)\n",
        "        \n",
        "    return {\n",
        "        'highly_correlated_pairs': highly_correlated,\n",
        "        'features_to_remove': to_remove,\n",
        "        'features_to_keep': all_features - to_remove\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: {('a','b'): 0.95}, thresh=0.9\n",
        "# 1. highly_correlated = {('a','b')}\n",
        "# 2. Loop ('a','b'): neither in to_remove -> add 'b' to to_remove\n",
        "# 3. features_to_keep = {'a', 'b'} - {'b'} = {'a'}\n",
        "\n",
        "# Test your solution\n",
        "correlations = {\n",
        "    ('f1', 'f2'): 0.95,\n",
        "    ('f1', 'f3'): 0.5,\n",
        "    ('f2', 'f4'): 0.92,\n",
        "    ('f3', 'f4'): 0.3\n",
        "}\n",
        "print(filter_correlated(correlations, threshold=0.9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section C: Dictionaries (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 11: Hyperparameter Grid Search ðŸŸ¢\n",
        "\n",
        "**Topic**: Dictionary basics and nested structures\n",
        "\n",
        "**Description**:\n",
        "Generate all combinations of hyperparameters for grid search:\n",
        "- Input: Dictionary where keys are parameter names and values are lists of options\n",
        "- Output: List of dictionaries, each representing one parameter combination\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "params = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "\n",
        "generate_grid(params)\n",
        "# Output: [\n",
        "#     {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 10},\n",
        "#     {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 20},\n",
        "#     {'learning_rate': 0.01, 'batch_size': 64, 'epochs': 10},\n",
        "#     ... (8 combinations total)\n",
        "# ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total combinations: 8\n",
            "First 3: [{'learning_rate': 0.01, 'batch_size': 32, 'epochs': 10}, {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 20}, {'learning_rate': 0.01, 'batch_size': 64, 'epochs': 10}]\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "from itertools import product\n",
        "\n",
        "def generate_grid(params):\n",
        "    \"\"\"\n",
        "    Generates all combinations of hyperparameters for grid search.\n",
        "    \n",
        "    Args:\n",
        "        params (dict): Dictionary of parameter names -> list of values\n",
        "        \n",
        "    Returns:\n",
        "        list: List of dictionaries, each a unique combination\n",
        "    \"\"\"\n",
        "    keys = list(params.keys())\n",
        "    values = list(params.values())\n",
        "    \n",
        "    # itertools.product generates Cartesian product of input iterables\n",
        "    # *values unpacks the lists of options\n",
        "    combinations = product(*values)\n",
        "    \n",
        "    # Reconstruct dictionary for each combination\n",
        "    result = []\n",
        "    for combo in combinations:\n",
        "        # zip pairs keys with the values in the current combination\n",
        "        result.append(dict(zip(keys, combo)))\n",
        "        \n",
        "    return result\n",
        "\n",
        "# Dry Run:\n",
        "# Input: {'a': [1, 2], 'b': [3]}\n",
        "# 1. keys=['a', 'b'], values=[[1, 2], [3]]\n",
        "# 2. product(*values) -> product([1, 2], [3]) -> (1, 3), (2, 3)\n",
        "# 3. zip(['a', 'b'], (1, 3)) -> {'a': 1, 'b': 3}\n",
        "# 4. zip(['a', 'b'], (2, 3)) -> {'a': 2, 'b': 3}\n",
        "# Result: [{'a': 1, 'b': 3}, {'a': 2, 'b': 3}]\n",
        "\n",
        "# Test your solution\n",
        "params = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "result = generate_grid(params)\n",
        "print(f\"Total combinations: {len(result)}\")\n",
        "print(\"First 3:\", result[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 12: Feature Statistics Calculator ðŸŸ¡\n",
        "\n",
        "**Topic**: Dictionary operations and statistics\n",
        "\n",
        "**Description**:\n",
        "Calculate statistics for each feature in a dataset:\n",
        "- Input: Dictionary where keys are feature names and values are lists of numbers\n",
        "- Output: Dictionary with nested statistics for each feature:\n",
        "  - 'mean', 'median', 'std', 'min', 'max', 'q1', 'q3'\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "data = {\n",
        "    'age': [25, 30, 35, 40, 45],\n",
        "    'income': [50000, 60000, 70000, 80000, 90000]\n",
        "}\n",
        "\n",
        "calculate_stats(data)\n",
        "# Output: {\n",
        "#     'age': {'mean': 35.0, 'median': 35.0, 'std': 7.07, 'min': 25, 'max': 45, 'q1': 30.0, 'q3': 40.0},\n",
        "#     'income': {'mean': 70000.0, 'median': 70000.0, 'std': 14142.14, 'min': 50000, 'max': 90000, 'q1': 60000.0, 'q3': 80000.0}\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'age': {'mean': 35, 'median': 35, 'std': 7.91, 'min': 25, 'max': 45, 'q1': 30.0, 'q3': 40.0}, 'income': {'mean': 70000, 'median': 70000, 'std': 15811.39, 'min': 50000, 'max': 90000, 'q1': 60000.0, 'q3': 80000.0}}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import statistics\n",
        "\n",
        "def calculate_stats(data):\n",
        "    \"\"\"\n",
        "    Calculates statistical metrics for each feature.\n",
        "    \n",
        "    Args:\n",
        "        data (dict): Feature name -> List of values\n",
        "        \n",
        "    Returns:\n",
        "        dict: Nested dictionary with stats per feature\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for feature, values in data.items():\n",
        "        if not values:\n",
        "            continue\n",
        "            \n",
        "        # Sort for quantile calculation\n",
        "        sorted_vals = sorted(values)\n",
        "        n = len(values)\n",
        "        \n",
        "        # Helper to find quantile (manual implementation to match typical requirements)\n",
        "        def get_quantile(sorted_list, q):\n",
        "            idx = q * (len(sorted_list) - 1)\n",
        "            lower = int(idx)\n",
        "            weight = idx - lower\n",
        "            upper = lower + 1\n",
        "            if upper >= len(sorted_list): return sorted_list[lower]\n",
        "            return sorted_list[lower] * (1 - weight) + sorted_list[upper] * weight\n",
        "\n",
        "        # Standard deviation sample calculation requires n > 1\n",
        "        std_val = statistics.stdev(values) if n > 1 else 0\n",
        "\n",
        "        stats_dict = {\n",
        "            'mean': statistics.mean(values),\n",
        "            'median': statistics.median(values),\n",
        "            'std': round(std_val, 2),\n",
        "            'min': min(values),\n",
        "            'max': max(values),\n",
        "            'q1': get_quantile(sorted_vals, 0.25),\n",
        "            'q3': get_quantile(sorted_vals, 0.75)\n",
        "        }\n",
        "        results[feature] = stats_dict\n",
        "        \n",
        "    return results\n",
        "\n",
        "# Dry Run:\n",
        "# Input: {'age': [10, 20, 30]}\n",
        "# 1. mean = 20\n",
        "# 2. median = 20\n",
        "# 3. min=10, max=30\n",
        "# 4. q1 (25%): idx=0.5 -> avg of pos 0 and 1 -> 15.0\n",
        "# Result: {'age': {'mean': 20, ...}}\n",
        "\n",
        "# Test your solution\n",
        "data = {\n",
        "    'age': [25, 30, 35, 40, 45],\n",
        "    'income': [50000, 60000, 70000, 80000, 90000]\n",
        "}\n",
        "print(calculate_stats(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 13: Data Aggregator ðŸŸ¡\n",
        "\n",
        "**Topic**: Dictionary aggregation and grouping\n",
        "\n",
        "**Description**:\n",
        "Group and aggregate data by category:\n",
        "- Input: List of dictionaries, each with 'category' and 'value' keys\n",
        "- Output: Dictionary where:\n",
        "  - Keys are categories\n",
        "  - Values are dictionaries with: 'count', 'sum', 'avg', 'values_list'\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "data = [\n",
        "    {'category': 'A', 'value': 10},\n",
        "    {'category': 'B', 'value': 20},\n",
        "    {'category': 'A', 'value': 15},\n",
        "    {'category': 'B', 'value': 25},\n",
        "    {'category': 'A', 'value': 12}\n",
        "]\n",
        "\n",
        "aggregate_data(data)\n",
        "# Output: {\n",
        "#     'A': {'count': 3, 'sum': 37, 'avg': 12.33, 'values_list': [10, 15, 12]},\n",
        "#     'B': {'count': 2, 'sum': 45, 'avg': 22.5, 'values_list': [20, 25]}\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'A': {'count': 3, 'sum': 37, 'avg': 12.33, 'values_list': [10, 15, 12]}, 'B': {'count': 2, 'sum': 45, 'avg': 22.5, 'values_list': [20, 25]}}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def aggregate_data(data):\n",
        "    \"\"\"\n",
        "    Aggregates data by category.\n",
        "    \n",
        "    Args:\n",
        "        data (list): List of dicts [{'category': 'A', 'value': 10}, ...]\n",
        "        \n",
        "    Returns:\n",
        "        dict: Aggregated stats by category\n",
        "    \"\"\"\n",
        "    grouped = {}\n",
        "    \n",
        "    # Step 1: Group values by category\n",
        "    for item in data:\n",
        "        cat = item['category']\n",
        "        val = item['value']\n",
        "        if cat not in grouped:\n",
        "            grouped[cat] = []\n",
        "        grouped[cat].append(val)\n",
        "        \n",
        "    # Step 2: Calculate aggregates for each group\n",
        "    result = {}\n",
        "    for cat, values in grouped.items():\n",
        "        result[cat] = {\n",
        "            'count': len(values),\n",
        "            'sum': sum(values),\n",
        "            'avg': round(sum(values) / len(values), 2),\n",
        "            'values_list': values\n",
        "        }\n",
        "        \n",
        "    return result\n",
        "\n",
        "# Dry Run:\n",
        "# Input: [{'c': 'A', 'v': 1}, {'c': 'A', 'v': 3}]\n",
        "# 1. Grouping: 'A' -> [1, 3]\n",
        "# 2. Aggregating 'A':\n",
        "#    count = 2\n",
        "#    sum = 4\n",
        "#    avg = 2.0\n",
        "# Result: {'A': {'count': 2, 'sum': 4, 'avg': 2.0, ...}}\n",
        "\n",
        "# Test your solution\n",
        "data = [\n",
        "    {'category': 'A', 'value': 10},\n",
        "    {'category': 'B', 'value': 20},\n",
        "    {'category': 'A', 'value': 15},\n",
        "    {'category': 'B', 'value': 25},\n",
        "    {'category': 'A', 'value': 12}\n",
        "]\n",
        "print(aggregate_data(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 14: Confusion Matrix Builder ðŸ”´\n",
        "\n",
        "**Topic**: Nested dictionaries and ML metrics\n",
        "\n",
        "**Description**:\n",
        "Build a confusion matrix from predictions and actual values:\n",
        "- Input: Two lists (actual, predicted) of same length\n",
        "- Output: Dictionary with:\n",
        "  - `'matrix'`: Nested dict {actual: {predicted: count}}\n",
        "  - `'accuracy'`: Overall accuracy\n",
        "  - `'per_class'`: Dict with precision, recall, f1 for each class\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "actual = ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'C']\n",
        "predicted = ['A', 'B', 'B', 'C', 'A', 'A', 'C', 'B']\n",
        "\n",
        "build_confusion_matrix(actual, predicted)\n",
        "# Output: {\n",
        "#     'matrix': {\n",
        "#         'A': {'A': 2, 'B': 1},\n",
        "#         'B': {'B': 1, 'A': 1},\n",
        "#         'C': {'C': 2, 'B': 1}\n",
        "#     },\n",
        "#     'accuracy': 0.625,  # 5 correct out of 8\n",
        "#     'per_class': {\n",
        "#         'A': {'precision': 0.5, 'recall': 0.67, 'f1': 0.57},\n",
        "#         'B': {'precision': 0.33, 'recall': 0.5, 'f1': 0.4},\n",
        "#         'C': {'precision': 1.0, 'recall': 0.67, 'f1': 0.8}\n",
        "#     }\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'matrix': {'A': {'A': 2, 'B': 1, 'C': 0}, 'B': {'A': 1, 'B': 1, 'C': 0}, 'C': {'A': 0, 'B': 1, 'C': 2}}, 'accuracy': 0.625, 'per_class': {'A': {'precision': 0.67, 'recall': 0.67, 'f1': 0.67}, 'B': {'precision': 0.33, 'recall': 0.5, 'f1': 0.4}, 'C': {'precision': 1.0, 'recall': 0.67, 'f1': 0.8}}}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def build_confusion_matrix(actual, predicted):\n",
        "    \"\"\"\n",
        "    Constructs a confusion matrix and calculates classification metrics.\n",
        "    \n",
        "    Args:\n",
        "        actual (list): True class labels\n",
        "        predicted (list): Predicted class labels\n",
        "        \n",
        "    Returns:\n",
        "        dict: Matrix and metrics\n",
        "    \"\"\"\n",
        "    # Get all unique classes\n",
        "    classes = sorted(list(set(actual) | set(predicted)))\n",
        "    \n",
        "    # Initialize matrix with 0s\n",
        "    matrix = {c: {c2: 0 for c2 in classes} for c in classes}\n",
        "    \n",
        "    correct_count = 0\n",
        "    total = len(actual)\n",
        "    \n",
        "    # Fill matrix: row=actual, col=predicted\n",
        "    for a, p in zip(actual, predicted):\n",
        "        matrix[a][p] += 1\n",
        "        if a == p:\n",
        "            correct_count += 1\n",
        "            \n",
        "    # Calculate per-class metrics\n",
        "    per_class = {}\n",
        "    for c in classes:\n",
        "        # True Positive: Actual=c, Predicted=c\n",
        "        tp = matrix[c][c]\n",
        "        \n",
        "        # False Positive: Predicted=c, but Actual!=c (sum column c excluding diagonal)\n",
        "        fp = sum(matrix[cx][c] for cx in classes if cx != c)\n",
        "        \n",
        "        # False Negative: Actual=c, but Predicted!=c (sum row c excluding diagonal)\n",
        "        fn = sum(matrix[c][px] for px in classes if px != c)\n",
        "        \n",
        "        # Metric formulas\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        \n",
        "        per_class[c] = {\n",
        "            'precision': round(precision, 2),\n",
        "            'recall': round(recall, 2),\n",
        "            'f1': round(f1, 2)\n",
        "        }\n",
        "    \n",
        "    return {\n",
        "        'matrix': matrix,\n",
        "        'accuracy': correct_count / total,\n",
        "        'per_class': per_class\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: Actual=['A'], Pred=['A']\n",
        "# 1. classes=['A']\n",
        "# 2. matrix={'A': {'A': 1}}\n",
        "# 3. TP=1, FP=0, FN=0\n",
        "# 4. Precision=1.0, Recall=1.0, F1=1.0\n",
        "# 5. Accuracy = 1/1 = 1.0\n",
        "\n",
        "# Test your solution\n",
        "actual = ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'C']\n",
        "predicted = ['A', 'B', 'B', 'C', 'A', 'A', 'C', 'B']\n",
        "print(build_confusion_matrix(actual, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 15: Configuration Merger ðŸ”´\n",
        "\n",
        "**Topic**: Nested dictionary operations\n",
        "\n",
        "**Description**:\n",
        "Merge multiple configuration dictionaries with priority rules:\n",
        "- Input: List of config dicts (later configs override earlier ones)\n",
        "- Handle nested dictionaries (deep merge)\n",
        "- Output: Single merged configuration\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "configs = [\n",
        "    {'model': {'layers': 3, 'dropout': 0.5}, 'lr': 0.01},\n",
        "    {'model': {'layers': 5}, 'batch_size': 32},\n",
        "    {'model': {'dropout': 0.3}, 'lr': 0.001}\n",
        "]\n",
        "\n",
        "merge_configs(configs)\n",
        "# Output: {\n",
        "#     'model': {'layers': 5, 'dropout': 0.3},\n",
        "#     'lr': 0.001,\n",
        "#     'batch_size': 32\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': {'layers': 5, 'dropout': 0.3}, 'lr': 0.001, 'batch_size': 32}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def merge_configs(configs):\n",
        "    \"\"\"\n",
        "    Recursively merges a list of configuration dictionaries.\n",
        "    Later configs override earlier ones.\n",
        "    \n",
        "    Args:\n",
        "        configs (list): List of dicts\n",
        "        \n",
        "    Returns:\n",
        "        dict: Merged dictionary\n",
        "    \"\"\"\n",
        "    merged = {}\n",
        "    \n",
        "    def deep_update(target, source):\n",
        "        for k, v in source.items():\n",
        "            # If both target and source values are dicts, recurse\n",
        "            if k in target and isinstance(target[k], dict) and isinstance(v, dict):\n",
        "                deep_update(target[k], v)\n",
        "            else:\n",
        "                # Otherwise overwrite\n",
        "                target[k] = v\n",
        "        return target\n",
        "    \n",
        "    for config in configs:\n",
        "        deep_update(merged, config)\n",
        "        \n",
        "    return merged\n",
        "\n",
        "# Dry Run:\n",
        "# Input: [{'a': 1}, {'a': 2, 'b': 3}]\n",
        "# 1. Start with {}\n",
        "# 2. Merge {'a': 1} -> merged={'a': 1}\n",
        "# 3. Merge {'a': 2, 'b': 3}:\n",
        "#    - 'a': exists, update to 2\n",
        "#    - 'b': new, set to 3\n",
        "# Result: {'a': 2, 'b': 3}\n",
        "\n",
        "# Test your solution\n",
        "configs = [\n",
        "    {'model': {'layers': 3, 'dropout': 0.5}, 'lr': 0.01},\n",
        "    {'model': {'layers': 5}, 'batch_size': 32},\n",
        "    {'model': {'dropout': 0.3}, 'lr': 0.001}\n",
        "]\n",
        "print(merge_configs(configs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Python Functions & Functional Programming (10 Problems)\n",
        "\n",
        "## Section D: Functions (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 16: Decorator - Execution Timer ðŸŸ¡\n",
        "\n",
        "**Topic**: Function decorators\n",
        "\n",
        "**Description**:\n",
        "Create a decorator that measures function execution time:\n",
        "- Decorator should print: function name, execution time in milliseconds\n",
        "- Should work with any function\n",
        "- Return the original function's result\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "@timer\n",
        "def slow_function(n):\n",
        "    time.sleep(n)\n",
        "    return n * 2\n",
        "\n",
        "result = slow_function(0.5)\n",
        "# Output: \"slow_function executed in 500.23 ms\"\n",
        "# result = 1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slow_function executed in 503.77 ms\n",
            "Result: 1.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import time\n",
        "import functools\n",
        "\n",
        "def timer(func):\n",
        "    \"\"\"\n",
        "    Decorator that measures execution time of a function.\n",
        "    \"\"\"\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        # Record start time\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Execute actual function\n",
        "        result = func(*args, **kwargs)\n",
        "        \n",
        "        # Record end time\n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Calculate duration in ms\n",
        "        execution_time = (end_time - start_time) * 1000\n",
        "        print(f\"{func.__name__} executed in {execution_time:.2f} ms\")\n",
        "        \n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Dry Run:\n",
        "# Call slow_function(0.1)\n",
        "# 1. start = T0\n",
        "# 2. run function (sleeps 0.1s)\n",
        "# 3. end = T0 + 0.1s\n",
        "# 4. print \"executed in 100.00 ms\"\n",
        "# 5. return result\n",
        "\n",
        "# Test your decorator\n",
        "@timer\n",
        "def slow_function(n):\n",
        "    time.sleep(n)\n",
        "    return n * 2\n",
        "\n",
        "result = slow_function(0.5)\n",
        "print(f\"Result: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 17: Memoization Cache ðŸ”´\n",
        "\n",
        "**Topic**: Closures and caching\n",
        "\n",
        "**Description**:\n",
        "Create a memoization decorator for expensive functions:\n",
        "- Cache function results based on arguments\n",
        "- Return cached result if same arguments are used again\n",
        "- Track cache hits and misses\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "@memoize\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "print(fibonacci(10))  # Computes and caches\n",
        "print(fibonacci(10))  # Returns from cache\n",
        "print(fibonacci.cache_info())  # {'hits': 1, 'misses': 11, 'size': 11}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First call: 55\n",
            "Second call: 55\n",
            "Cache info: {'hits': 9, 'misses': 11, 'size': 11}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import functools\n",
        "\n",
        "def memoize(func):\n",
        "    \"\"\"\n",
        "    Decorator that caches function results (memoization).\n",
        "    \"\"\"\n",
        "    cache = {}\n",
        "    stats = {'hits': 0, 'misses': 0}\n",
        "    \n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args):\n",
        "        # Check if arguments are already in cache\n",
        "        if args in cache:\n",
        "            stats['hits'] += 1\n",
        "            return cache[args]\n",
        "        \n",
        "        # If not, compute result\n",
        "        stats['misses'] += 1\n",
        "        result = func(*args)\n",
        "        \n",
        "        # Store in cache\n",
        "        cache[args] = result\n",
        "        return result\n",
        "    \n",
        "    # Helper to inspect cache performance\n",
        "    def cache_info():\n",
        "        return {**stats, 'size': len(cache)}\n",
        "    \n",
        "    wrapper.cache_info = cache_info\n",
        "    return wrapper\n",
        "\n",
        "# Dry Run:\n",
        "# Call fib(1) -> miss, compute, store { (1,): 1 }\n",
        "# Call fib(1) -> hit, return 1\n",
        "# Call fib(2) -> miss, compute fib(1)+fib(0), store { (2,): 1 }\n",
        "\n",
        "# Test your decorator\n",
        "@memoize\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "print(\"First call:\", fibonacci(10))\n",
        "print(\"Second call:\", fibonacci(10))\n",
        "print(\"Cache info:\", fibonacci.cache_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 18: Partial Function Application ðŸŸ¡\n",
        "\n",
        "**Topic**: Closures and function factories\n",
        "\n",
        "**Description**:\n",
        "Create a function factory for ML model evaluators:\n",
        "- Input: Metric name ('accuracy', 'precision', 'recall', 'f1')\n",
        "- Output: Function that calculates that metric given y_true and y_pred\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "accuracy_fn = create_metric('accuracy')\n",
        "precision_fn = create_metric('precision')\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0]\n",
        "y_pred = [1, 0, 1, 0, 0]\n",
        "\n",
        "print(accuracy_fn(y_true, y_pred))  # 0.8 (4/5 correct)\n",
        "print(precision_fn(y_true, y_pred))  # 1.0 (all predicted 1s are correct)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8\n",
            "Precision: 1.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def create_metric(metric_name):\n",
        "    \"\"\"\n",
        "    Factory function producing specific metric calculation functions.\n",
        "    \"\"\"\n",
        "    def accuracy(y_true, y_pred):\n",
        "        # Count correct matches / total\n",
        "        correct = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
        "        return correct / len(y_true)\n",
        "        \n",
        "    def precision(y_true, y_pred):\n",
        "        # Precision = TP / (TP + FP)\n",
        "        # TP: true=1, pred=1\n",
        "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
        "        # FP: true=0, pred=1\n",
        "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
        "        return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        \n",
        "    if metric_name == 'accuracy':\n",
        "        return accuracy\n",
        "    elif metric_name == 'precision':\n",
        "        return precision\n",
        "    else:\n",
        "        raise ValueError(\"Unknown metric\")\n",
        "\n",
        "# Dry Run:\n",
        "# create_metric('accuracy') returns the inner 'accuracy' function.\n",
        "# Calling that with [1,0], [1,1]:\n",
        "# 1. (1,1) match -> correct=1\n",
        "# 2. (0,1) mismatch\n",
        "# 3. Result 1/2 = 0.5\n",
        "\n",
        "# Test your solution\n",
        "accuracy_fn = create_metric('accuracy')\n",
        "precision_fn = create_metric('precision')\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0]\n",
        "y_pred = [1, 0, 1, 0, 0]\n",
        "\n",
        "print(\"Accuracy:\", accuracy_fn(y_true, y_pred))\n",
        "print(\"Precision:\", precision_fn(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 19: Pipeline Builder ðŸ”´\n",
        "\n",
        "**Topic**: Function composition and *args\n",
        "\n",
        "**Description**:\n",
        "Create a pipeline that applies multiple functions in sequence:\n",
        "- Input: Variable number of functions\n",
        "- Output: Single function that applies all functions in order\n",
        "- Data flows from left to right\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "def normalize(x):\n",
        "    return [i / max(x) for i in x]\n",
        "\n",
        "def square(x):\n",
        "    return [i ** 2 for i in x]\n",
        "\n",
        "def sum_all(x):\n",
        "    return sum(x)\n",
        "\n",
        "pipeline = create_pipeline(normalize, square, sum_all)\n",
        "result = pipeline([1, 2, 3, 4, 5])\n",
        "# normalize: [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "# square: [0.04, 0.16, 0.36, 0.64, 1.0]\n",
        "# sum_all: 2.2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: 2.2\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def create_pipeline(*functions):\n",
        "    \"\"\"\n",
        "    Composes a list of functions into a single pipeline.\n",
        "    Data flows: Input -> F1 -> F2 -> F3 -> Output\n",
        "    \"\"\"\n",
        "    def pipeline(data):\n",
        "        result = data\n",
        "        for func in functions:\n",
        "            # Pass output of previous function as input to next\n",
        "            result = func(result)\n",
        "        return result\n",
        "    return pipeline\n",
        "\n",
        "# Dry Run:\n",
        "# Pipeline(f1, f2) called with x\n",
        "# 1. result = f1(x)\n",
        "# 2. result = f2(result)\n",
        "# 3. return result\n",
        "\n",
        "# Test your solution\n",
        "def normalize(x):\n",
        "    return [i / max(x) for i in x]\n",
        "\n",
        "def square(x):\n",
        "    return [i ** 2 for i in x]\n",
        "\n",
        "def sum_all(x):\n",
        "    return sum(x)\n",
        "\n",
        "pipeline = create_pipeline(normalize, square, sum_all)\n",
        "result = pipeline([1, 2, 3, 4, 5])\n",
        "# Flow: [1..5] -> Norm -> [0.2..1.0] -> Sq -> [0.04..1.0] -> Sum -> 2.2\n",
        "print(f\"Result: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 20: Currying ðŸ”´\n",
        "\n",
        "**Topic**: Currying and partial application\n",
        "\n",
        "**Description**:\n",
        "Transform a function that takes multiple arguments into a sequence of functions each taking a single argument:\n",
        "- Input: Function with N parameters\n",
        "- Output: Curried version that can be called step by step\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "def add_three(a, b, c):\n",
        "    return a + b + c\n",
        "\n",
        "curried_add = curry(add_three)\n",
        "result = curried_add(1)(2)(3)  # 6\n",
        "\n",
        "# Or partial application\n",
        "add_1 = curried_add(1)\n",
        "add_1_2 = add_1(2)\n",
        "result = add_1_2(3)  # 6\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Curried: 6\n",
            "Partial application: 6\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import inspect\n",
        "\n",
        "def curry(func):\n",
        "    \"\"\"\n",
        "    Transforms a function taking N args into N functions taking 1 arg.\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(func)\n",
        "    # Determine how many arguments the function expects\n",
        "    num_args = len(sig.parameters)\n",
        "    \n",
        "    def curried(*args):\n",
        "        # If we have enough arguments, call the original function\n",
        "        if len(args) >= num_args:\n",
        "            return func(*args)\n",
        "        # Otherwise return a lambda expecting the rest of the arguments\n",
        "        # The new lambda will recursively call 'curried' accumulating args\n",
        "        return lambda *next_args: curried(*(args + next_args))\n",
        "        \n",
        "    return curried\n",
        "\n",
        "# Dry Run:\n",
        "# add(a,b). num_args=2.\n",
        "# 1. curried(1) -> len(1) < 2 -> returns lambda expecting next\n",
        "# 2. lambda(2) calls curried(1, 2)\n",
        "# 3. curried(1, 2) -> len(2) >= 2 -> calls add(1, 2) -> 3\n",
        "\n",
        "# Test your solution\n",
        "def add_three(a, b, c):\n",
        "    return a + b + c\n",
        "\n",
        "curried_add = curry(add_three)\n",
        "result1 = curried_add(1)(2)(3)\n",
        "print(f\"Curried: {result1}\")\n",
        "\n",
        "add_1 = curried_add(1)\n",
        "add_1_2 = add_1(2)\n",
        "result2 = add_1_2(3)\n",
        "print(f\"Partial application: {result2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section E: Functional Programming (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 21: Data Transformation Pipeline ðŸŸ¢\n",
        "\n",
        "**Topic**: map, filter, lambda\n",
        "\n",
        "**Description**:\n",
        "Transform raw data using functional programming:\n",
        "- Input: List of strings representing numbers: `['1', '2', '3', '4', '5', '6']`\n",
        "- Tasks:\n",
        "  1. Convert to integers\n",
        "  2. Filter even numbers only\n",
        "  3. Square each number\n",
        "  4. Return sum\n",
        "- Use map, filter, lambda (no loops!)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "transform_data(['1', '2', '3', '4', '5', '6'])\n",
        "# Steps: ['1','2','3','4','5','6'] -> [1,2,3,4,5,6] -> [2,4,6] -> [4,16,36] -> 56\n",
        "# Output: 56\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: 56\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def transform_data(data):\n",
        "    \"\"\"\n",
        "    Transforms list of number strings: to int -> filter even -> square -> sum.\n",
        "    \n",
        "    Args:\n",
        "        data (list): List of strings representing numbers\n",
        "        \n",
        "    Returns:\n",
        "        int: Sum of squared even numbers\n",
        "    \"\"\"\n",
        "    # Functional approach using map/filter/reduce (sum is a reduction)\n",
        "    \n",
        "    # 1. map(int, data): Converts strings to integers\n",
        "    integers = map(int, data)\n",
        "    \n",
        "    # 2. filter(lambda x: x % 2 == 0): Keeps only even numbers\n",
        "    evens = filter(lambda x: x % 2 == 0, integers)\n",
        "    \n",
        "    # 3. map(lambda x: x**2): Squares the numbers\n",
        "    squared = map(lambda x: x**2, evens)\n",
        "    \n",
        "    # 4. sum(): Reducer that adds them all up\n",
        "    return sum(squared)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: ['1', '2', '3', '4']\n",
        "# 1. map(int) -> [1, 2, 3, 4]\n",
        "# 2. filter(even) -> [2, 4]\n",
        "# 3. map(square) -> [4, 16]\n",
        "# 4. sum() -> 4 + 16 = 20\n",
        "\n",
        "# Test your solution\n",
        "result = transform_data(['1', '2', '3', '4', '5', '6'])\n",
        "print(f\"Result: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 22: Feature Scaler ðŸŸ¡\n",
        "\n",
        "**Topic**: Lambda and list comprehensions\n",
        "\n",
        "**Description**:\n",
        "Create different scaling functions using lambdas:\n",
        "- Min-Max scaling: `(x - min) / (max - min)`\n",
        "- Z-score normalization: `(x - mean) / std`\n",
        "- Robust scaling: `(x - median) / IQR`\n",
        "- Input: List of numbers and scaling type\n",
        "- Output: Scaled list\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "data = [1, 2, 3, 4, 5]\n",
        "\n",
        "scale_features(data, 'minmax')\n",
        "# Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "scale_features(data, 'zscore')\n",
        "# Output: [-1.41, -0.71, 0.0, 0.71, 1.41]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min-Max: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
            "Z-score: [-1.26, -0.63, 0.0, 0.63, 1.26]\n",
            "Robust: [-1.0, -0.5, 0.0, 0.5, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import statistics\n",
        "\n",
        "def scale_features(data, method):\n",
        "    \"\"\"\n",
        "    Scales numerical features using different techniques.\n",
        "    \n",
        "    Args:\n",
        "        data (list): List of numerical values\n",
        "        method (str): 'minmax', 'zscore', or 'robust'\n",
        "        \n",
        "    Returns:\n",
        "        list: Scaled values\n",
        "    \"\"\"\n",
        "    if not data: return []\n",
        "    \n",
        "    if method == 'minmax':\n",
        "        # Formula: (x - min) / (max - min)\n",
        "        min_val = min(data)\n",
        "        max_val = max(data)\n",
        "        rng = max_val - min_val\n",
        "        return list(map(lambda x: (x - min_val) / rng if rng != 0 else 0, data))\n",
        "        \n",
        "    elif method == 'zscore':\n",
        "        # Formula: (x - mean) / std\n",
        "        mean = statistics.mean(data)\n",
        "        # Handle single element case where std is undefined or 0\n",
        "        if len(data) > 1:\n",
        "            std = statistics.stdev(data)\n",
        "        else:\n",
        "            std = 0\n",
        "            \n",
        "        return list(map(lambda x: round((x - mean) / std, 2) if std != 0 else 0, data))\n",
        "        \n",
        "    elif method == 'robust':\n",
        "        # Formula: (x - median) / IQR\n",
        "        sorted_data = sorted(data)\n",
        "        n = len(data)\n",
        "        median = statistics.median(data)\n",
        "        \n",
        "        # Simple quartile approximation\n",
        "        q1 = sorted_data[int(0.25 * n)]\n",
        "        q3 = sorted_data[int(0.75 * n)]\n",
        "        iqr = q3 - q1\n",
        "        \n",
        "        return list(map(lambda x: (x - median) / iqr if iqr != 0 else 0, data))\n",
        "\n",
        "# Dry Run (MinMax):\n",
        "# Input: [10, 20, 30]\n",
        "# 1. min=10, max=30, rng=20\n",
        "# 2. x=10 -> (10-10)/20 = 0.0\n",
        "# 3. x=20 -> (20-10)/20 = 0.5\n",
        "# 4. x=30 -> (30-10)/20 = 1.0\n",
        "# Result: [0.0, 0.5, 1.0]\n",
        "\n",
        "# Test your solution\n",
        "data = [1, 2, 3, 4, 5]\n",
        "print(\"Min-Max:\", scale_features(data, 'minmax'))\n",
        "print(\"Z-score:\", scale_features(data, 'zscore'))\n",
        "print(\"Robust:\", scale_features(data, 'robust'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 23: reduce() - Custom Aggregations ðŸŸ¡\n",
        "\n",
        "**Topic**: reduce() for complex aggregations\n",
        "\n",
        "**Description**:\n",
        "Use `reduce()` to implement custom aggregation functions:\n",
        "1. Product of all elements\n",
        "2. Maximum value\n",
        "3. Concatenate strings with separator\n",
        "4. Flatten nested lists\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "product([1, 2, 3, 4, 5])  # 120\n",
        "maximum([3, 1, 4, 1, 5, 9, 2, 6])  # 9\n",
        "join_strings(['Hello', 'World', 'ML'], ' ')  # 'Hello World ML'\n",
        "flatten([[1, 2], [3, 4], [5, 6]])  # [1, 2, 3, 4, 5, 6]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product: 120\n",
            "Maximum: 9\n",
            "Join: Hello World ML\n",
            "Flatten: [1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "from functools import reduce\n",
        "\n",
        "def product(numbers):\n",
        "    # Multiply x * y cumulatively. Initializer=1 (identity for multiplication)\n",
        "    return reduce(lambda x, y: x * y, numbers, 1)\n",
        "\n",
        "def maximum(numbers):\n",
        "    # Determine max between current accum and next item\n",
        "    return reduce(lambda x, y: x if x > y else y, numbers)\n",
        "\n",
        "def join_strings(strings, separator):\n",
        "    # Concatenate string with separator\n",
        "    return reduce(lambda x, y: x + separator + y, strings)\n",
        "\n",
        "def flatten(nested_list):\n",
        "    # Concatenate lists. Initializer=[]\n",
        "    return reduce(lambda x, y: x + y, nested_list, [])\n",
        "\n",
        "# Dry Run (Product):\n",
        "# Input: [2, 3, 4]\n",
        "# 1. start=1\n",
        "# 2. 1 * 2 = 2\n",
        "# 3. 2 * 3 = 6\n",
        "# 4. 6 * 4 = 24\n",
        "\n",
        "# Test your solutions\n",
        "print(\"Product:\", product([1, 2, 3, 4, 5]))\n",
        "print(\"Maximum:\", maximum([3, 1, 4, 1, 5, 9, 2, 6]))\n",
        "print(\"Join:\", join_strings(['Hello', 'World', 'ML'], ' '))\n",
        "print(\"Flatten:\", flatten([[1, 2], [3, 4], [5, 6]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 24: Comprehension Olympics ðŸ”´\n",
        "\n",
        "**Topic**: Nested comprehensions and conditional logic\n",
        "\n",
        "**Description**:\n",
        "Solve these using comprehensions (no loops!):\n",
        "\n",
        "1. **Matrix Transpose**: Transpose a 2D matrix\n",
        "2. **Flatten with Condition**: Flatten matrix but only include even numbers\n",
        "3. **Cartesian Product**: All pairs from two lists where sum > 5\n",
        "4. **Dict Inversion**: Invert dict (values become keys), handle duplicates\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "transpose(matrix)  # [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n",
        "\n",
        "flatten_even(matrix)  # [2, 4, 6, 8]\n",
        "\n",
        "cartesian([1, 2, 3], [4, 5, 6])  # [(2, 4), (2, 5), (2, 6), (3, 4), (3, 5), (3, 6)]\n",
        "\n",
        "invert_dict({'a': 1, 'b': 2, 'c': 1})  # {1: ['a', 'c'], 2: ['b']}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transpose: [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n",
            "Flatten even: [2, 4, 6, 8]\n",
            "Cartesian: [(1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 4), (3, 5), (3, 6)]\n",
            "Invert: {1: ['a', 'c'], 2: ['b']}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def transpose(matrix):\n",
        "    # Swap rows and cols: row i col j becomes row j col i\n",
        "    return [[row[i] for row in matrix] for i in range(len(matrix[0]))]\n",
        "\n",
        "def flatten_even(matrix):\n",
        "    # Double loop comprehension with condition\n",
        "    return [num for row in matrix for num in row if num % 2 == 0]\n",
        "\n",
        "def cartesian(list1, list2, threshold=5):\n",
        "    # Double loop with filtering condition\n",
        "    return [(x, y) for x in list1 for y in list2 if x + y > threshold]\n",
        "\n",
        "def invert_dict(d):\n",
        "    # Invert mapping: value becomes key.\n",
        "    # Since values might not be unique, we collect keys in a list\n",
        "    unique_vals = set(d.values())\n",
        "    return {v: [k for k, val in d.items() if val == v] for v in unique_vals}\n",
        "\n",
        "# Dry Run (Transpose):\n",
        "# Input: [[1, 2], [3, 4]]\n",
        "# 1. i=0 -> [row[0] for row in matrix] -> [1, 3]\n",
        "# 2. i=1 -> [row[1] for row in matrix] -> [2, 4]\n",
        "# Result: [[1, 3], [2, 4]]\n",
        "\n",
        "# Test your solutions\n",
        "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "print(\"Transpose:\", transpose(matrix))\n",
        "print(\"Flatten even:\", flatten_even(matrix))\n",
        "print(\"Cartesian:\", cartesian([1, 2, 3], [4, 5, 6]))\n",
        "print(\"Invert:\", invert_dict({'a': 1, 'b': 2, 'c': 1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 25: Function Combinator ðŸ”´\n",
        "\n",
        "**Topic**: Higher-order functions\n",
        "\n",
        "**Description**:\n",
        "Create function combinators:\n",
        "1. `compose`: Combine functions right-to-left: `compose(f, g, h)(x)` = `f(g(h(x)))`\n",
        "2. `pipe`: Combine functions left-to-right: `pipe(f, g, h)(x)` = `h(g(f(x)))`\n",
        "3. `parallel`: Apply multiple functions and collect results: `parallel(f, g)(x)` = `[f(x), g(x)]`\n",
        "4. `conditional`: Apply function based on predicate: `conditional(pred, f, g)(x)` = `f(x) if pred(x) else g(x)`\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "add_2 = lambda x: x + 2\n",
        "mult_3 = lambda x: x * 3\n",
        "square = lambda x: x ** 2\n",
        "\n",
        "compose(square, mult_3, add_2)(5)  # square(mult_3(add_2(5))) = square(21) = 441\n",
        "pipe(add_2, mult_3, square)(5)  # square(mult_3(add_2(5))) = square(21) = 441\n",
        "parallel(add_2, mult_3, square)(5)  # [7, 15, 25]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compose: 441\n",
            "Pipe: 441\n",
            "Parallel: [7, 15, 25]\n",
            "Conditional: 7\n",
            "Conditional: -15\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "def compose(*functions):\n",
        "    # Right-to-left execution: f(g(x))\n",
        "    return lambda x: reduce(lambda acc, f: f(acc), reversed(functions), x)\n",
        "\n",
        "def pipe(*functions):\n",
        "    # Left-to-right execution: f(x) -> g(result)\n",
        "    return lambda x: reduce(lambda acc, f: f(acc), functions, x)\n",
        "\n",
        "def parallel(*functions):\n",
        "    # Execute all functions on same input and return list of results\n",
        "    return lambda x: [f(x) for f in functions]\n",
        "\n",
        "def conditional(predicate, true_func, false_func):\n",
        "    # If p(x) is true, run true_func(x), else false_func(x)\n",
        "    return lambda x: true_func(x) if predicate(x) else false_func(x)\n",
        "\n",
        "# Dry Run (Compose):\n",
        "# Input: f=square, g=inc. compose(f, g)(2)\n",
        "# 1. reversed -> [inc, square] (Actually compose is usually right-to-left order of args)\n",
        "# 2. Wait, standard math composition (f o g)(x) = f(g(x)).\n",
        "#    If args are (square, inc), then reversed is (inc, square).\n",
        "#    Start with x=2.\n",
        "#    a. inc(2) = 3\n",
        "#    b. square(3) = 9\n",
        "# Result: 9\n",
        "\n",
        "# Test your solutions\n",
        "add_2 = lambda x: x + 2\n",
        "mult_3 = lambda x: x * 3\n",
        "square = lambda x: x ** 2\n",
        "\n",
        "print(\"Compose:\", compose(square, mult_3, add_2)(5))\n",
        "print(\"Pipe:\", pipe(add_2, mult_3, square)(5))\n",
        "print(\"Parallel:\", parallel(add_2, mult_3, square)(5))\n",
        "print(\"Conditional:\", conditional(lambda x: x > 0, add_2, mult_3)(5))\n",
        "print(\"Conditional:\", conditional(lambda x: x > 0, add_2, mult_3)(-5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Pandas Basics & Data Manipulation (15 Problems)\n",
        "\n",
        "## Section F: Series & DataFrames (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 26: Series Statistics Calculator ðŸŸ¢\n",
        "\n",
        "**Topic**: Series creation and basic operations\n",
        "\n",
        "**Description**:\n",
        "Create a Pandas Series from a list and calculate comprehensive statistics:\n",
        "- Input: List of numbers\n",
        "- Output: Dictionary with: mean, median, std, var, min, max, q1, q3, skew\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "series_stats([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "# Output: {'mean': 5.5, 'median': 5.5, 'std': 3.03, 'var': 9.17, 'min': 1.0, 'max': 10.0, 'q1': 3.25, 'q3': 7.75, 'skew': 0.0}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mean': np.float64(5.5), 'median': np.float64(5.5), 'std': np.float64(3.03), 'var': np.float64(9.17), 'min': np.int64(1), 'max': np.int64(10), 'q1': np.float64(3.25), 'q3': np.float64(7.75), 'skew': np.float64(0.0)}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def series_stats(data):\n",
        "    \"\"\"\n",
        "    Converts list to Series and calculates detailed statistics.\n",
        "    \n",
        "    Args:\n",
        "        data (list): Numerical data\n",
        "        \n",
        "    Returns:\n",
        "        dict: Statistical summary\n",
        "    \"\"\"\n",
        "    s = pd.Series(data)\n",
        "    \n",
        "    # Calculate stats using Series methods\n",
        "    return {\n",
        "        'mean': round(s.mean(), 2),\n",
        "        'median': s.median(),\n",
        "        'std': round(s.std(), 2),\n",
        "        'var': round(s.var(), 2),\n",
        "        'min': s.min(),\n",
        "        'max': s.max(),\n",
        "        'q1': s.quantile(0.25),\n",
        "        'q3': s.quantile(0.75),\n",
        "        'skew': round(s.skew(), 2)\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: [1, 2, 3]\n",
        "# 1. Series: [1, 2, 3]\n",
        "# 2. mean=2.0, median=2.0\n",
        "# 3. std=1.0, var=1.0\n",
        "# ...\n",
        "# Result: dictionary of stats\n",
        "\n",
        "# Test your solution\n",
        "result = series_stats([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 27: DataFrame from Multiple Sources ðŸŸ¢\n",
        "\n",
        "**Topic**: DataFrame creation from different data structures\n",
        "\n",
        "**Description**:\n",
        "Create a function that builds a DataFrame from multiple input formats:\n",
        "- Dictionary of lists\n",
        "- List of dictionaries\n",
        "- List of tuples with column names\n",
        "- Dictionary of Series\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# From dict of lists\n",
        "create_dataframe({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
        "\n",
        "# From list of dicts\n",
        "create_dataframe([{'A': 1, 'B': 4}, {'A': 2, 'B': 5}, {'A': 3, 'B': 6}])\n",
        "\n",
        "# Both produce:\n",
        "#    A  B\n",
        "# 0  1  4\n",
        "# 1  2  5\n",
        "# 2  3  6\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method 1: Dict of lists\n",
            "   A  B\n",
            "0  1  4\n",
            "1  2  5\n",
            "2  3  6\n",
            "\n",
            "\n",
            "   A  B\n",
            "0  1  4\n",
            "1  2  5\n",
            "2  3  6\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def create_dataframe(data, columns=None):\n",
        "    \"\"\"\n",
        "    Creates a DataFrame from various input formats.\n",
        "    \n",
        "    Args:\n",
        "        data: input data (dict of lists, list of dicts, etc.)\n",
        "        columns: column names (optional)\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame\n",
        "    \"\"\"\n",
        "    # Pandas constructor is very flexible and handles most of these cases automatically\n",
        "    # But let's be explicit for the logic\n",
        "    \n",
        "    if isinstance(data, dict):\n",
        "        # Case 1: Dict of lists {'A': [1], 'B': [2]}\n",
        "        return pd.DataFrame(data)\n",
        "        \n",
        "    elif isinstance(data, list):\n",
        "        if len(data) > 0 and isinstance(data[0], dict):\n",
        "            # Case 2: List of dicts [{'A':1}, {'A':2}]\n",
        "            return pd.DataFrame(data)\n",
        "        elif params := columns:\n",
        "            # Case 3: List of lists/tuples with column names\n",
        "            return pd.DataFrame(data, columns=columns)\n",
        "            \n",
        "    # Fallback to default constructor\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: {'A': [1]}\n",
        "# 1. isinstance(dict) -> True\n",
        "# 2. pd.DataFrame({'A': [1]})\n",
        "# Result: DataFrame with 1 row, col A\n",
        "\n",
        "# Test your solution\n",
        "print(\"Method 1: Dict of lists\")\n",
        "print(create_dataframe({'A': [1, 2, 3], 'B': [4, 5, 6]}))\n",
        "\n",
        "print(\"\\n\")\n",
        "#Method 2: List of dicts\")\n",
        "print(create_dataframe([{'A': 1, 'B': 4}, {'A': 2, 'B': 5}, {'A': 3, 'B': 6}]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 28: Custom Index Creator ðŸŸ¡\n",
        "\n",
        "**Topic**: DataFrame indexing and custom indices\n",
        "\n",
        "**Description**:\n",
        "Create a DataFrame with custom index based on requirements:\n",
        "- Input: Data and index type ('numeric', 'date', 'custom')\n",
        "- For 'numeric': Start from 100, step by 5\n",
        "- For 'date': Daily dates from '2024-01-01'\n",
        "- For 'custom': Use provided list as index\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "data = {'Score': [85, 90, 78, 92, 88]}\n",
        "\n",
        "create_with_index(data, 'numeric')\n",
        "# Index: [100, 105, 110, 115, 120]\n",
        "\n",
        "create_with_index(data, 'date')\n",
        "# Index: DatetimeIndex(['2024-01-01', '2024-01-02', ...])\n",
        "\n",
        "create_with_index(data, 'custom', custom_index=['A', 'B', 'C', 'D', 'E'])\n",
        "# Index: ['A', 'B', 'C', 'D', 'E']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numeric index:\n",
            "RangeIndex(start=100, stop=125, step=5)\n",
            "\n",
            "Date index:\n",
            "DatetimeIndex(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',\n",
            "               '2024-01-05'],\n",
            "              dtype='datetime64[us]', freq='D')\n",
            "\n",
            "Custom index:\n",
            "Index(['A', 'B', 'C', 'D', 'E'], dtype='str')\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def create_with_index(data, index_type, custom_index=None):\n",
        "    \"\"\"\n",
        "    Creates DataFrame with specific index types.\n",
        "    \n",
        "    Args:\n",
        "        data (dict): Data for dataframe\n",
        "        index_type (str): 'numeric', 'date', or 'custom'\n",
        "        custom_index (list): List for custom index\n",
        "        \n",
        "    Returns:\n",
        "        pd.Index: The resulting index object\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(data)\n",
        "    n_rows = len(df)\n",
        "    \n",
        "    if index_type == 'numeric':\n",
        "        # Start at 100, step 5\n",
        "        df.index = range(100, 100 + n_rows * 5, 5)\n",
        "        \n",
        "    elif index_type == 'date':\n",
        "        # Date range starting 2024-01-01\n",
        "        df.index = pd.date_range(start='2024-01-01', periods=n_rows, freq='D')\n",
        "        \n",
        "    elif index_type == 'custom' and custom_index:\n",
        "        df.index = custom_index\n",
        "        \n",
        "    return df.index\n",
        "\n",
        "# Dry Run:\n",
        "# Input: {'A': [1,2]}, index_type='numeric'\n",
        "# 1. n_rows = 2\n",
        "# 2. range(100, 100 + 2*5, 5) -> range(100, 110, 5) -> [100, 105]\n",
        "# Result: Int64Index([100, 105], ...)\n",
        "\n",
        "# Test your solution\n",
        "data = {'Score': [85, 90, 78, 92, 88]}\n",
        "\n",
        "print(\"Numeric index:\")\n",
        "print(create_with_index(data, 'numeric'))\n",
        "\n",
        "print(\"\\nDate index:\")\n",
        "print(create_with_index(data, 'date'))\n",
        "\n",
        "print(\"\\nCustom index:\")\n",
        "print(create_with_index(data, 'custom', custom_index=['A', 'B', 'C', 'D', 'E']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 29: DataFrame Info Extractor ðŸŸ¡\n",
        "\n",
        "**Topic**: DataFrame inspection and metadata\n",
        "\n",
        "**Description**:\n",
        "Extract comprehensive information about a DataFrame:\n",
        "- Shape, column names, dtypes, memory usage\n",
        "- Missing values count per column\n",
        "- Unique values count per column\n",
        "- Basic statistics for numeric columns\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'Age': [25, 30, None, 35, 40],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'Salary': [50000, 60000, 70000, None, 90000]\n",
        "})\n",
        "\n",
        "extract_info(df)\n",
        "# Output: {\n",
        "#     'shape': (5, 3),\n",
        "#     'columns': ['Age', 'Name', 'Salary'],\n",
        "#     'dtypes': {'Age': 'float64', 'Name': 'object', 'Salary': 'float64'},\n",
        "#     'missing_values': {'Age': 1, 'Name': 0, 'Salary': 1},\n",
        "#     'unique_counts': {'Age': 4, 'Name': 5, 'Salary': 4},\n",
        "#     'memory_mb': 0.00024,\n",
        "#     'numeric_summary': {...}\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame Info:\n",
            "shape: (5, 3)\n",
            "columns: ['Age', 'Name', 'Salary']\n",
            "dtypes: {'Age': 'float64', 'Name': 'str', 'Salary': 'float64'}\n",
            "missing_values: {'Age': 1, 'Name': 0, 'Salary': 1}\n",
            "unique_counts: {'Age': 4, 'Name': 5, 'Salary': 4}\n",
            "memory_mb: 0.00046\n",
            "numeric_summary: {'Age': {'count': 4.0, 'mean': 32.5, 'std': 6.454972243679028, 'min': 25.0, '25%': 28.75, '50%': 32.5, '75%': 36.25, 'max': 40.0}, 'Salary': {'count': 4.0, 'mean': 67500.0, 'std': 17078.25127659933, 'min': 50000.0, '25%': 57500.0, '50%': 65000.0, '75%': 75000.0, 'max': 90000.0}}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def extract_info(df):\n",
        "    \"\"\"\n",
        "    Extracts comprehensive metadata from a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        \n",
        "    Returns:\n",
        "        dict: Metadata dictionary\n",
        "    \"\"\"\n",
        "    # Calculate memory usage in MB\n",
        "    mem_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
        "    \n",
        "    return {\n",
        "        'shape': df.shape,\n",
        "        'columns': df.columns.tolist(),\n",
        "        'dtypes': df.dtypes.astype(str).to_dict(),\n",
        "        'missing_values': df.isnull().sum().to_dict(),\n",
        "        'unique_counts': df.nunique().to_dict(),\n",
        "        'memory_mb': round(mem_usage, 5),\n",
        "        'numeric_summary': df.describe().to_dict()\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: DF shape (2,1), col 'A'=[1, None]\n",
        "# 1. shape -> (2,1)\n",
        "# 2. missing -> {'A': 1}\n",
        "# 3. unique -> {'A': 1} (None doesn't count by default usually, depends on pandas version)\n",
        "# 4. Result: Dictionary populated\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'Age': [25, 30, None, 35, 40],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'Salary': [50000, 60000, 70000, None, 90000]\n",
        "})\n",
        "\n",
        "info = extract_info(df)\n",
        "print(\"DataFrame Info:\")\n",
        "for key, value in info.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 30: Row and Column Analyzer ðŸ”´\n",
        "\n",
        "**Topic**: Advanced DataFrame operations\n",
        "\n",
        "**Description**:\n",
        "Analyze rows and columns of a DataFrame:\n",
        "- Find rows with any/all missing values\n",
        "- Find columns with >50% missing data\n",
        "- Identify duplicate rows\n",
        "- Find numeric columns that are highly correlated (>0.9)\n",
        "- Return detailed analysis dictionary\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4, 5],\n",
        "    'B': [None, None, None, None, 5],\n",
        "    'C': [1, 2, 3, 4, 5],\n",
        "    'D': [1.1, 2.1, 3.1, 4.1, 5.1]  # Highly correlated with C\n",
        "})\n",
        "\n",
        "analyze_dataframe(df)\n",
        "# Output: {\n",
        "#     'rows_with_missing': [0, 1, 2, 3],\n",
        "#     'rows_all_missing': [],\n",
        "#     'cols_over_50_missing': ['B'],\n",
        "#     'duplicate_rows': [],\n",
        "#     'highly_correlated_pairs': [('C', 'D', 0.999)]\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame Analysis:\n",
            "rows_with_missing: [0, 1, 2, 3]\n",
            "rows_all_missing: []\n",
            "cols_over_50_missing: ['B']\n",
            "duplicate_rows: []\n",
            "highly_correlated_pairs: [('A', 'C', np.float64(1.0)), ('A', 'D', np.float64(1.0)), ('C', 'D', np.float64(1.0))]\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_dataframe(df):\n",
        "    \"\"\"\n",
        "    Performs data quality analysis on a DataFrame.\n",
        "    Identifies missing data patterns and correlations.\n",
        "    \"\"\"\n",
        "    # Correlation analysis\n",
        "    # Select only numeric columns\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    corr_matrix = numeric_df.corr().abs()\n",
        "    \n",
        "    # Select upper triangle of correlation matrix (excluding diagonal)\n",
        "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    \n",
        "    # Find pairs with correlation > 0.9\n",
        "    high_corr_tuples = []\n",
        "    for column in upper_tri.columns:\n",
        "        for idx in upper_tri.index:\n",
        "            val = upper_tri.loc[idx, column]\n",
        "            if val > 0.9:\n",
        "                high_corr_tuples.append((idx, column, round(val, 3)))\n",
        "\n",
        "    return {\n",
        "        # Rows where ANY column is missing\n",
        "        'rows_with_missing': df[df.isnull().any(axis=1)].index.tolist(),\n",
        "        \n",
        "        # Rows where ALL columns are missing\n",
        "        'rows_all_missing': df[df.isnull().all(axis=1)].index.tolist(),\n",
        "        \n",
        "        # Columns with > 50% missing\n",
        "        'cols_over_50_missing': df.columns[df.isnull().mean() > 0.5].tolist(),\n",
        "        \n",
        "        # Duplicate rows\n",
        "        'duplicate_rows': df[df.duplicated()].index.tolist(),\n",
        "        \n",
        "        # Correlated features\n",
        "        'highly_correlated_pairs': high_corr_tuples\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: DF 3 rows. Row 0 complete. Row 1 has None. Row 2 duplicate of Row 0.\n",
        "# 1. rows_with_missing -> [1]\n",
        "# 2. duplicate_rows -> [2]\n",
        "# Result: {'rows_with_missing': [1], 'duplicate_rows': [2], ...}\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4, 5],\n",
        "    'B': [None, None, None, None, 5],\n",
        "    'C': [1, 2, 3, 4, 5],\n",
        "    'D': [1.1, 2.1, 3.1, 4.1, 5.1]\n",
        "})\n",
        "\n",
        "analysis = analyze_dataframe(df)\n",
        "print(\"DataFrame Analysis:\")\n",
        "for key, value in analysis.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section G: Data Selection & Filtering (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 31: Multi-Column Selector ðŸŸ¢\n",
        "\n",
        "**Topic**: Column selection techniques\n",
        "\n",
        "**Description**:\n",
        "Select columns from DataFrame based on different criteria:\n",
        "- By data type (numeric, object, datetime)\n",
        "- By name pattern (contains, starts with, ends with)\n",
        "- By value threshold (mean > threshold)\n",
        "- Return new DataFrame with selected columns\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, 35],\n",
        "    'name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'salary': [50000, 60000, 70000],\n",
        "    'bonus': [5000, 6000, 7000]\n",
        "})\n",
        "\n",
        "select_columns(df, method='numeric')\n",
        "# Returns: DataFrame with ['age', 'salary', 'bonus']\n",
        "\n",
        "select_columns(df, method='contains', pattern='sal')\n",
        "# Returns: DataFrame with ['salary']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numeric columns:\n",
            "   age  salary  bonus\n",
            "0   25   50000   5000\n",
            "1   30   60000   6000\n",
            "2   35   70000   7000\n",
            "\n",
            "Columns containing 'sal':\n",
            "   salary\n",
            "0   50000\n",
            "1   60000\n",
            "2   70000\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def select_columns(df, method='all', pattern=None, dtype=None):\n",
        "    \"\"\"\n",
        "    Flexibly selects columns from a DataFrame based on criteria.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        method: Selection method ('numeric', 'contains', 'starts_with', 'ends_with', 'threshold')\n",
        "        pattern: String pattern (for string methods) or threshold value (for 'threshold')\n",
        "        dtype: Data type (for 'threshold' method)\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with selected columns\n",
        "    \"\"\"\n",
        "    if method == 'numeric':\n",
        "        # Select integer and float columns\n",
        "        return df.select_dtypes(include=['number'])\n",
        "        \n",
        "    elif method == 'contains':\n",
        "        # Select columns whose names contain 'pattern'\n",
        "        return df.filter(like=pattern)\n",
        "        \n",
        "    elif method == 'starts_with':\n",
        "        # Regex anchor ^ for start of string\n",
        "        return df.filter(regex=f\"^{pattern}\")\n",
        "        \n",
        "    elif method == 'ends_with':\n",
        "        # Regex anchor $ for end of string\n",
        "        return df.filter(regex=f\"{pattern}$\")\n",
        "        \n",
        "    elif method == 'threshold' and dtype:\n",
        "        # Custom logic: columns of specific type with mean > pattern\n",
        "        numerics = df.select_dtypes(include=['number'])\n",
        "        # loc[:, mask] selects all rows, but only columns where mask is True\n",
        "        return numerics.loc[:, numerics.mean() > pattern]\n",
        "        \n",
        "    return df\n",
        "\n",
        "# Dry Run:\n",
        "# Input: Columns ['age', 'name'], method='contains', pattern='na'\n",
        "# 1. df.filter(like='na') -> Matches 'name'\n",
        "# Result: DataFrame with 'name' column\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, 35],\n",
        "    'name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'salary': [50000, 60000, 70000],\n",
        "    'bonus': [5000, 6000, 7000]\n",
        "})\n",
        "\n",
        "print(\"Numeric columns:\")\n",
        "print(select_columns(df, method='numeric'))\n",
        "\n",
        "print(\"\\nColumns containing 'sal':\")\n",
        "print(select_columns(df, method='contains', pattern='sal'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 32: Advanced Row Filtering ðŸŸ¡\n",
        "\n",
        "**Topic**: Complex boolean indexing\n",
        "\n",
        "**Description**:\n",
        "Filter DataFrame rows using multiple conditions:\n",
        "- Single condition (column > value)\n",
        "- AND condition (col1 > val1 AND col2 < val2)\n",
        "- OR condition (col1 > val1 OR col2 > val2)\n",
        "- BETWEEN condition (val1 < col < val2)\n",
        "- IN condition (col.isin([list]))\n",
        "- String matching (col.str.contains())\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'age': [25, 30, 35, 40],\n",
        "    'city': ['NYC', 'LA', 'NYC', 'Chicago'],\n",
        "    'salary': [50000, 60000, 70000, 80000]\n",
        "})\n",
        "\n",
        "filter_rows(df, conditions={'age': ('>', 30), 'city': ('==', 'NYC')}, logic='AND')\n",
        "# Returns rows where age > 30 AND city == 'NYC'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age > 30:\n",
            "      name  age     city  salary\n",
            "2  Charlie   35      NYC   70000\n",
            "3    David   40  Chicago   80000\n",
            "\n",
            "Age > 30 AND city == NYC:\n",
            "      name  age city  salary\n",
            "2  Charlie   35  NYC   70000\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def filter_rows(df, conditions, logic='AND'):\n",
        "    \"\"\"\n",
        "    Filters rows based on dictionary of conditions.\n",
        "    \n",
        "    Args:\n",
        "        conditions (dict): {col: (operator, value)}\n",
        "        logic: 'AND' or 'OR'\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "    \n",
        "    for col, (op, val) in conditions.items():\n",
        "        # Construct query strings for df.query()\n",
        "        if op == '>':\n",
        "            queries.append(f\"`{col}` > {val}\")\n",
        "        elif op == '<':\n",
        "            queries.append(f\"`{col}` < {val}\")\n",
        "        elif op == '==':\n",
        "            if isinstance(val, str):\n",
        "                queries.append(f\"`{col}` == '{val}'\")\n",
        "            else:\n",
        "                queries.append(f\"`{col}` == {val}\")\n",
        "    \n",
        "    # Join all separate conditions with logic operator\n",
        "    if logic == 'AND':\n",
        "        query_str = \" and \".join(queries)\n",
        "    else:\n",
        "        query_str = \" or \".join(queries)\n",
        "        \n",
        "    return df.query(query_str)\n",
        "\n",
        "# Dry Run:\n",
        "# Input logic='AND', conditions={'age': ('>', 20), 'city': ('==', 'NYC')}\n",
        "# 1. Query parts: [\"`age` > 20\", \"`city` == 'NYC'\"]\n",
        "# 2. Joined: \"`age` > 20 and `city` == 'NYC'\"\n",
        "# 3. df.query -> returns filtered dataframe\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'age': [25, 30, 35, 40],\n",
        "    'city': ['NYC', 'LA', 'NYC', 'Chicago'],\n",
        "    'salary': [50000, 60000, 70000, 80000]\n",
        "})\n",
        "\n",
        "print(\"Age > 30:\")\n",
        "print(filter_rows(df, {'age': ('>', 30)}, logic='AND'))\n",
        "\n",
        "print(\"\\nAge > 30 AND city == NYC:\")\n",
        "print(filter_rows(df, {'age': ('>', 30), 'city': ('==', 'NYC')}, logic='AND'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 33: Loc vs iLoc Master ðŸŸ¡\n",
        "\n",
        "**Topic**: Label-based and position-based indexing\n",
        "\n",
        "**Description**:\n",
        "Practice both .loc and .iloc for various selection tasks:\n",
        "1. Select specific rows and columns by label\n",
        "2. Select specific rows and columns by position\n",
        "3. Select rows based on condition and specific columns\n",
        "4. Select every nth row\n",
        "5. Select last 3 rows and first 2 columns\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [10, 20, 30, 40, 50],\n",
        "    'C': [100, 200, 300, 400, 500]\n",
        "}, index=['row1', 'row2', 'row3', 'row4', 'row5'])\n",
        "\n",
        "# Using .loc\n",
        "select_data(df, method='loc', rows=['row1', 'row3'], cols=['A', 'C'])\n",
        "# Returns:\n",
        "#       A    C\n",
        "# row1  1  100\n",
        "# row3  3  300\n",
        "\n",
        "# Using .iloc\n",
        "select_data(df, method='iloc', rows=[0, 2], cols=[0, 2])\n",
        "# Same result but using positions\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using .loc:\n",
            "      A    C\n",
            "row1  1  100\n",
            "row3  3  300\n",
            "\n",
            "Using .iloc:\n",
            "      A    C\n",
            "row1  1  100\n",
            "row3  3  300\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def select_data(df, method='loc', rows=None, cols=None):\n",
        "    \"\"\"\n",
        "    Selects data using loc (label-based) or iloc (position-based).\n",
        "    \"\"\"\n",
        "    if method == 'loc':\n",
        "        # Uses labels for rows and columns\n",
        "        return df.loc[rows, cols]\n",
        "    elif method == 'iloc':\n",
        "        # Uses integer positions\n",
        "        return df.iloc[rows, cols]\n",
        "\n",
        "# Dry Run:\n",
        "# Input: method='iloc', rows=[0], cols=[0]\n",
        "# 1. df.iloc[[0], [0]] -> Element at (0,0) as DataFrame/Series\n",
        "# Result: Subset of data\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [10, 20, 30, 40, 50],\n",
        "    'C': [100, 200, 300, 400, 500]\n",
        "}, index=['row1', 'row2', 'row3', 'row4', 'row5'])\n",
        "\n",
        "print(\"Using .loc:\")\n",
        "print(select_data(df, method='loc', rows=['row1', 'row3'], cols=['A', 'C']))\n",
        "\n",
        "print(\"\\nUsing .iloc:\")\n",
        "print(select_data(df, method='iloc', rows=[0, 2], cols=[0, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 34: Query Method Expert ðŸŸ¡\n",
        "\n",
        "**Topic**: DataFrame.query() for readable filtering\n",
        "\n",
        "**Description**:\n",
        "Use the .query() method to filter DataFrames with string expressions:\n",
        "- Simple comparisons\n",
        "- Multiple conditions with 'and', 'or'\n",
        "- Use of external variables with @\n",
        "- String column operations\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, 35, 40, 45],\n",
        "    'salary': [50000, 60000, 70000, 80000, 90000],\n",
        "    'department': ['IT', 'HR', 'IT', 'Sales', 'IT']\n",
        "})\n",
        "\n",
        "threshold = 65000\n",
        "\n",
        "filter_with_query(df, \"age > 30 and salary > @threshold\")\n",
        "# Returns rows where age > 30 and salary > 65000\n",
        "\n",
        "filter_with_query(df, \"department == 'IT'\")\n",
        "# Returns all IT department rows\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age > 30 and salary > threshold:\n",
            "   age  salary department\n",
            "2   35   70000         IT\n",
            "3   40   80000      Sales\n",
            "4   45   90000         IT\n",
            "\n",
            "Department == 'IT':\n",
            "   age  salary department\n",
            "0   25   50000         IT\n",
            "2   35   70000         IT\n",
            "4   45   90000         IT\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def filter_with_query(df, query_string):\n",
        "    \"\"\"\n",
        "    Wraps pandas query method.\n",
        "    Allows using @variable syntax in query string.\n",
        "    \"\"\"\n",
        "    return df.query(query_string)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: \"age > 30\"\n",
        "# Result: Rows where age > 30\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, 35, 40, 45],\n",
        "    'salary': [50000, 60000, 70000, 80000, 90000],\n",
        "    'department': ['IT', 'HR', 'IT', 'Sales', 'IT']\n",
        "})\n",
        "\n",
        "threshold = 65000\n",
        "\n",
        "print(\"Age > 30 and salary > threshold:\")\n",
        "print(filter_with_query(df, \"age > 30 and salary > @threshold\"))\n",
        "\n",
        "print(\"\\nDepartment == 'IT':\")\n",
        "print(filter_with_query(df, \"department == 'IT'\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 35: Top N Selector ðŸ”´\n",
        "\n",
        "**Topic**: nlargest, nsmallest, and ranking\n",
        "\n",
        "**Description**:\n",
        "Create a function that finds top/bottom N rows based on multiple criteria:\n",
        "- Top N by single column\n",
        "- Top N by multiple columns (with priority)\n",
        "- Bottom N with ties handling\n",
        "- Percentage-based selection (top 20%)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'score': [85, 92, 88, 92, 78],\n",
        "    'time': [120, 100, 110, 95, 130]\n",
        "})\n",
        "\n",
        "select_top_n(df, n=2, column='score', method='largest')\n",
        "# Returns:\n",
        "#      name  score  time\n",
        "# 1     Bob     92   100\n",
        "# 3   David     92    95\n",
        "\n",
        "select_top_n(df, n=2, columns=['score', 'time'], method='largest', priority='score')\n",
        "# First sort by score (desc), then by time (desc)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 2 by score:\n",
            "    name  score  time\n",
            "1    Bob     92   100\n",
            "3  David     92    95\n",
            "\n",
            "Bottom 2 by time:\n",
            "    name  score  time\n",
            "3  David     92    95\n",
            "1    Bob     92   100\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def select_top_n(df, n=5, column=None, columns=None, method='largest', keep='all'):\n",
        "    \"\"\"\n",
        "    Selects top or bottom N rows based on values.\n",
        "    \n",
        "    Args:\n",
        "        n (int): Number of rows\n",
        "        column (str): Single column to sort by\n",
        "        columns (list): Multiple columns to sort by\n",
        "        method: 'largest' or 'smallest'\n",
        "        keep: Handling ties ('first', 'last', 'all')\n",
        "    \"\"\"\n",
        "    # If standard 1 column\n",
        "    if column:\n",
        "        if method == 'largest':\n",
        "            # nlargest uses specific optimization for obtaining top N\n",
        "            return df.nlargest(n, column, keep=keep)\n",
        "        else:\n",
        "            return df.nsmallest(n, column, keep=keep)\n",
        "            \n",
        "    # If multiple columns or specific sort needed (fallback to sort_values)\n",
        "    elif columns:\n",
        "        ascending = (method == 'smallest')\n",
        "        return df.sort_values(by=columns, ascending=ascending).head(n)\n",
        "        \n",
        "    return df.head(n)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: n=2, column='score', method='largest'\n",
        "# 1. df.nlargest(2, 'score')\n",
        "# Result: Top 2 rows with highest scores\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'score': [85, 92, 88, 92, 78],\n",
        "    'time': [120, 100, 110, 95, 130]\n",
        "})\n",
        "\n",
        "print(\"Top 2 by score:\")\n",
        "print(select_top_n(df, n=2, column='score', method='largest'))\n",
        "\n",
        "print(\"\\nBottom 2 by time:\")\n",
        "print(select_top_n(df, n=2, column='time', method='smallest'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section G: GroupBy & Aggregation (5 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 36: Basic GroupBy Aggregation ðŸŸ¢\n",
        "\n",
        "**Topic**: Single column groupby with simple aggregations\n",
        "\n",
        "**Description**:\n",
        "Group data by a column and apply aggregations:\n",
        "- Sum, mean, median, count, std, min, max\n",
        "- Custom aggregation functions\n",
        "- Return dictionary with aggregation results\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'category': ['A', 'B', 'A', 'B', 'A', 'C'],\n",
        "    'value': [10, 20, 30, 40, 50, 60],\n",
        "    'count': [1, 2, 3, 4, 5, 6]\n",
        "})\n",
        "\n",
        "group_and_aggregate(df, groupby='category', agg_col='value', agg_func='mean')\n",
        "# Output:\n",
        "# {\n",
        "#     'A': 30.0,\n",
        "#     'B': 30.0,\n",
        "#     'C': 60.0\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean by category:\n",
            "{'A': 30.0, 'B': 30.0, 'C': 60.0}\n",
            "\n",
            "Sum by category:\n",
            "{'A': 90, 'B': 60, 'C': 60}\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def group_and_aggregate(df, groupby, agg_col, agg_func='mean'):\n",
        "    \"\"\"\n",
        "    Performs basic groupby and aggregation.\n",
        "    \"\"\"\n",
        "    # groupby(col)[target].agg(func) returns a Series/DataFrame\n",
        "    # .to_dict() converts it to a dictionary for return\n",
        "    return df.groupby(groupby)[agg_col].agg(agg_func).to_dict()\n",
        "\n",
        "# Dry Run:\n",
        "# Input: group='cat', col='val', func='sum'\n",
        "# 1. Group A -> sum([10, 30]) = 40\n",
        "# 2. Group B -> sum([20]) = 20\n",
        "# Result: {'A': 40, 'B': 20}\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'category': ['A', 'B', 'A', 'B', 'A', 'C'],\n",
        "    'value': [10, 20, 30, 40, 50, 60],\n",
        "    'count': [1, 2, 3, 4, 5, 6]\n",
        "})\n",
        "\n",
        "print(\"Mean by category:\")\n",
        "print(group_and_aggregate(df, groupby='category', agg_col='value', agg_func='mean'))\n",
        "\n",
        "print(\"\\nSum by category:\")\n",
        "print(group_and_aggregate(df, groupby='category', agg_col='value', agg_func='sum'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 37: Multiple Aggregations ðŸŸ¡\n",
        "\n",
        "**Topic**: Apply multiple aggregation functions at once\n",
        "\n",
        "**Description**:\n",
        "Group by column and apply multiple aggregations to different columns:\n",
        "- Use .agg() with dictionary\n",
        "- Named aggregations\n",
        "- Return DataFrame with multi-level columns\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'department': ['IT', 'HR', 'IT', 'HR', 'Sales'],\n",
        "    'employee': ['A', 'B', 'C', 'D', 'E'],\n",
        "    'salary': [50000, 45000, 55000, 48000, 52000],\n",
        "    'experience': [5, 3, 7, 4, 6]\n",
        "})\n",
        "\n",
        "multi_agg(df, groupby='department')\n",
        "# Returns DataFrame with:\n",
        "#            salary              experience\n",
        "#              mean  max  min      mean  max\n",
        "# department                                \n",
        "# HR          46500  48000  45000  3.5    4\n",
        "# IT          52500  55000  50000  6.0    7\n",
        "# Sales       52000  52000  52000  6.0    6\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             salary               experience    \n",
            "               mean    max    min       mean max\n",
            "department                                      \n",
            "HR          46500.0  48000  45000        3.5   4\n",
            "IT          52500.0  55000  50000        6.0   7\n",
            "Sales       52000.0  52000  52000        6.0   6\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def multi_agg(df, groupby):\n",
        "    \"\"\"\n",
        "    Performs multiple aggregations on different columns.\n",
        "    \"\"\"\n",
        "    # pass dict to agg: {column: [list of functions]}\n",
        "    return df.groupby(groupby).agg({\n",
        "        'salary': ['mean', 'max', 'min'],\n",
        "        'experience': ['mean', 'max']\n",
        "    })\n",
        "\n",
        "# Dry Run:\n",
        "# Group by Dept.\n",
        "# Salary: mean, max, min\n",
        "# Experience: mean, max\n",
        "# Result: DataFrame with MultiIndex columns\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'department': ['IT', 'HR', 'IT', 'HR', 'Sales'],\n",
        "    'employee': ['A', 'B', 'C', 'D', 'E'],\n",
        "    'salary': [50000, 45000, 55000, 48000, 52000],\n",
        "    'experience': [5, 3, 7, 4, 6]\n",
        "})\n",
        "\n",
        "result = multi_agg(df, groupby='department')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 38: Transform and Apply ðŸŸ¡\n",
        "\n",
        "**Topic**: GroupBy transform and apply methods\n",
        "\n",
        "**Description**:\n",
        "Use transform and apply for group-wise operations:\n",
        "- Transform: Normalize values within each group (subtract group mean, divide by group std)\n",
        "- Apply: Custom function to each group\n",
        "- Return transformed DataFrame\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
        "    'value': [10, 20, 30, 40, 50, 60]\n",
        "})\n",
        "\n",
        "# Normalize within groups\n",
        "group_normalize(df, groupby='group', column='value')\n",
        "# Returns DataFrame with normalized values:\n",
        "#   group  value  normalized\n",
        "# 0     A     10       -1.0\n",
        "# 1     A     20        1.0\n",
        "# 2     B     30       -1.0\n",
        "# 3     B     40        1.0\n",
        "# 4     C     50       -1.0\n",
        "# 5     C     60        1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  group  value  normalized\n",
            "0     A     10   -0.707107\n",
            "1     A     20    0.707107\n",
            "2     B     30   -0.707107\n",
            "3     B     40    0.707107\n",
            "4     C     50   -0.707107\n",
            "5     C     60    0.707107\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def group_normalize(df, groupby, column):\n",
        "    \"\"\"\n",
        "    Normalizes a column within its group (Z-score normalization).\n",
        "    \"\"\"\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # Custom z-score function\n",
        "    def zscore(x):\n",
        "        # Handle zero division if std is 0 (constant values in group)\n",
        "        return (x - x.mean()) / x.std() if x.std() != 0 else 0\n",
        "        \n",
        "    # Transform applies the function to each group but preserves original index structure\n",
        "    df_new['normalized'] = df.groupby(groupby)[column].transform(zscore)\n",
        "    return df_new\n",
        "\n",
        "# Dry Run:\n",
        "# Group A: [10, 20]. Mean=15, Std=7.07\n",
        "# 10 -> (10-15)/7.07 = -0.707\n",
        "# 20 -> (20-15)/7.07 = 0.707\n",
        "# Result: DF with new 'normalized' column\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
        "    'value': [10, 20, 30, 40, 50, 60]\n",
        "})\n",
        "\n",
        "result = group_normalize(df, groupby='group', column='value')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 39: Pivot Table Creator ðŸŸ¡\n",
        "\n",
        "**Topic**: Pivot tables and cross-tabulation\n",
        "\n",
        "**Description**:\n",
        "Create pivot tables from DataFrames:\n",
        "- Simple pivot with one index and one column\n",
        "- Multiple aggregations\n",
        "- Add margins (totals)\n",
        "- Fill missing values\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'date': ['2024-01', '2024-01', '2024-02', '2024-02'],\n",
        "    'product': ['A', 'B', 'A', 'B'],\n",
        "    'sales': [100, 150, 120, 180],\n",
        "    'quantity': [10, 15, 12, 18]\n",
        "})\n",
        "\n",
        "create_pivot(df, index='date', columns='product', values='sales')\n",
        "# Returns:\n",
        "# product     A      B\n",
        "# date               \n",
        "# 2024-01   100    150\n",
        "# 2024-02   120    180\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pivot table:\n",
            "product    A    B\n",
            "date             \n",
            "2024-01  100  150\n",
            "2024-02  120  180\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def create_pivot(df, index, columns, values, aggfunc='sum', fill_value=None):\n",
        "    \"\"\"\n",
        "    Creates a pivot table.\n",
        "    \n",
        "    Args:\n",
        "        index: Column to be rows\n",
        "        columns: Column to be fields\n",
        "        values: Column to aggregate\n",
        "    \"\"\"\n",
        "    return df.pivot_table(index=index, columns=columns, values=values, \n",
        "                         aggfunc=aggfunc, fill_value=fill_value)\n",
        "\n",
        "# Dry Run:\n",
        "# Rows=Date, Cols=Product, Val=Sales\n",
        "# Row 2024-01: Prod A=100, Prod B=150\n",
        "# Result: 2D table\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'date': ['2024-01', '2024-01', '2024-02', '2024-02'],\n",
        "    'product': ['A', 'B', 'A', 'B'],\n",
        "    'sales': [100, 150, 120, 180],\n",
        "    'quantity': [10, 15, 12, 18]\n",
        "})\n",
        "\n",
        "print(\"Pivot table:\")\n",
        "print(create_pivot(df, index='date', columns='product', values='sales'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 40: Advanced GroupBy with Filtering ðŸ”´\n",
        "\n",
        "**Topic**: Complex groupby operations with filtering\n",
        "\n",
        "**Description**:\n",
        "Perform advanced groupby operations:\n",
        "- Filter groups based on group properties (size, sum, etc.)\n",
        "- Rank within groups\n",
        "- Calculate percentages within groups\n",
        "- Find top N within each group\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'store': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
        "    'product': ['X', 'Y', 'Z', 'X', 'Y', 'X', 'Y', 'Z', 'W'],\n",
        "    'sales': [100, 150, 200, 120, 180, 90, 110, 130, 140]\n",
        "})\n",
        "\n",
        "advanced_groupby(df, groupby='store', operations={\n",
        "    'filter_size': 3,  # Keep only groups with 3+ items\n",
        "    'top_n': 2,  # Top 2 products per store\n",
        "    'rank': True,  # Add rank within group\n",
        "    'pct': True  # Add percentage of total within group\n",
        "})\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  store product  sales  rank\n",
            "1     A       Y    150   2.0\n",
            "2     A       Z    200   1.0\n",
            "7     C       Z    130   2.0\n",
            "8     C       W    140   1.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def advanced_groupby(df, groupby, operations):\n",
        "    \"\"\"\n",
        "    Performs complex chain of operations on groups.\n",
        "    Operations: 'filter_size', 'rank', 'top_n'\n",
        "    \"\"\"\n",
        "    # Start with base groupby (used indirectly via filter/apply)\n",
        "    result = df.copy()\n",
        "    \n",
        "    # 1. Filter groups by size\n",
        "    if 'filter_size' in operations:\n",
        "        # filter() keeps rows belonging to groups that satisfy the lambda\n",
        "        result = result.groupby(groupby).filter(lambda x: len(x) >= operations['filter_size'])\n",
        "        \n",
        "    # 2. Calculate rank within group\n",
        "    if 'rank' in operations and operations['rank']:\n",
        "        # transform with rank()\n",
        "        result['rank'] = result.groupby(groupby)['sales'].rank(ascending=False)\n",
        "        \n",
        "    # 3. Filter top N per group\n",
        "    if 'top_n' in operations:\n",
        "        if 'rank' in result.columns:\n",
        "            # If we calculated rank, just filter by it (Rank 1 is highest)\n",
        "            result = result[result['rank'] <= operations['top_n']]\n",
        "        else:\n",
        "             # Fallback: group apply nlargest\n",
        "             result = result.groupby(groupby, group_keys=False).apply(\n",
        "                 lambda x: x.nlargest(operations['top_n'], 'sales')\n",
        "             )\n",
        "             \n",
        "    return result.sort_values(groupby)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: Groups A (size 3), B (size 2). Filter size 3.\n",
        "# 1. Filter: Keeps only A rows.\n",
        "# 2. Rank: A rows ranked by sales.\n",
        "# 3. Top N=2: Keep ranks 1 and 2 from A.\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'store': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
        "    'product': ['X', 'Y', 'Z', 'X', 'Y', 'X', 'Y', 'Z', 'W'],\n",
        "    'sales': [100, 150, 200, 120, 180, 90, 110, 130, 140]\n",
        "})\n",
        "\n",
        "operations = {'filter_size': 3, 'top_n': 2, 'rank': True}\n",
        "result = advanced_groupby(df, groupby='store', operations=operations)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Pandas Advanced Operations (10 Problems)\n",
        "\n",
        "## Section I: Missing Data (3 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 41: Missing Data Detector ðŸŸ¢\n",
        "\n",
        "**Topic**: Identifying missing values\n",
        "\n",
        "**Description**:\n",
        "Create comprehensive missing data analysis:\n",
        "- Count missing values per column\n",
        "- Percentage of missing values\n",
        "- Identify patterns (missing in pairs, etc.)\n",
        "- Visualize missing data (text-based representation)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4, 5],\n",
        "    'B': [None, 2, 3, None, 5],\n",
        "    'C': [1, 2, 3, 4, 5]\n",
        "})\n",
        "\n",
        "analyze_missing(df)\n",
        "# Output: {\n",
        "#     'total_missing': 3,\n",
        "#     'by_column': {'A': 1, 'B': 2, 'C': 0},\n",
        "#     'percentage': {'A': 20.0, 'B': 40.0, 'C': 0.0},\n",
        "#     'rows_with_missing': [0, 2, 3],\n",
        "#     'complete_rows': 2\n",
        "# }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Data Analysis:\n",
            "total_missing: 3\n",
            "by_column: {'A': 1, 'B': 2, 'C': 0}\n",
            "percentage: {'A': 20.0, 'B': 40.0, 'C': 0.0}\n",
            "rows_with_missing: [0, 2, 3]\n",
            "complete_rows: 2\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_missing(df):\n",
        "    \"\"\"\n",
        "    Analyzes missing data patterns.\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "    Returns:\n",
        "        dict: Missing data statistics\n",
        "    \"\"\"\n",
        "    missing_counts = df.isnull().sum()\n",
        "    total_rows = len(df)\n",
        "    \n",
        "    # Rows with ANY missing value\n",
        "    rows_with_any = df[df.isnull().any(axis=1)]\n",
        "    \n",
        "    return {\n",
        "        'total_missing': int(missing_counts.sum()),\n",
        "        'by_column': missing_counts.to_dict(),\n",
        "        'percentage': round((missing_counts / total_rows) * 100, 2).to_dict(),\n",
        "        'rows_with_missing': rows_with_any.index.tolist(),\n",
        "        'complete_rows': int(total_rows - len(rows_with_any))\n",
        "    }\n",
        "\n",
        "# Dry Run:\n",
        "# Input: DF shape(3,2). [1, Nan], [2, 2], [Nan, Nan]\n",
        "# 1. missing_counts: Col1=1, Col2=2. Total=3.\n",
        "# 2. rows_with_missing: Indices [0, 2]\n",
        "# 3. complete_rows: 3 - 2 = 1 (Index 1)\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4, 5],\n",
        "    'B': [None, 2, 3, None, 5],\n",
        "    'C': [1, 2, 3, 4, 5]\n",
        "})\n",
        "\n",
        "result = analyze_missing(df)\n",
        "print(\"Missing Data Analysis:\")\n",
        "for key, value in result.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 42: Smart Missing Value Imputer ðŸŸ¡\n",
        "\n",
        "**Topic**: Filling missing values with different strategies\n",
        "\n",
        "**Description**:\n",
        "Implement multiple imputation strategies:\n",
        "- Fill with mean/median/mode\n",
        "- Forward fill (ffill) / Backward fill (bfill)\n",
        "- Interpolate (linear, polynomial)\n",
        "- Fill with group mean (groupby then fill)\n",
        "- Fill with predictive model (simple linear regression)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'group': ['A', 'A', 'A', 'B', 'B', 'B'],\n",
        "    'value': [10, None, 30, 40, None, 60]\n",
        "})\n",
        "\n",
        "impute_missing(df, column='value', method='mean')\n",
        "# Fills with overall mean: 35\n",
        "\n",
        "impute_missing(df, column='value', method='group_mean', groupby='group')\n",
        "# Fills with group mean: A=20, B=50\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            "  group  value\n",
            "0     A   10.0\n",
            "1     A    NaN\n",
            "2     A   30.0\n",
            "3     B   40.0\n",
            "4     B    NaN\n",
            "5     B   60.0\n",
            "\n",
            "Impute with mean:\n",
            "  group  value\n",
            "0     A   10.0\n",
            "1     A   35.0\n",
            "2     A   30.0\n",
            "3     B   40.0\n",
            "4     B   35.0\n",
            "5     B   60.0\n",
            "\n",
            "Impute with group mean:\n",
            "  group  value\n",
            "0     A   10.0\n",
            "1     A   20.0\n",
            "2     A   30.0\n",
            "3     B   40.0\n",
            "4     B   50.0\n",
            "5     B   60.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def impute_missing(df, column, method='mean', groupby=None):\n",
        "    \"\"\"\n",
        "    Imputes missing values using various strategies.\n",
        "    \n",
        "    Args:\n",
        "        method: 'mean', 'median', or 'group_mean'\n",
        "        groupby: Column to group by for 'group_mean'\n",
        "    \"\"\"\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    if method == 'mean':\n",
        "        # Simple mean imputation\n",
        "        df_new[column] = df_new[column].fillna(df_new[column].mean())\n",
        "        \n",
        "    elif method == 'median':\n",
        "        # Simple median imputation\n",
        "        df_new[column] = df_new[column].fillna(df_new[column].median())\n",
        "        \n",
        "    elif method == 'group_mean' and groupby:\n",
        "        # Impute with mean of the group the row belongs to\n",
        "        # transform('mean') broadcasts the group mean to every row\n",
        "        df_new[column] = df_new[column].fillna(\n",
        "            df_new.groupby(groupby)[column].transform('mean')\n",
        "        )\n",
        "        \n",
        "    return df_new\n",
        "\n",
        "# Dry Run:\n",
        "# Input: Group A=[10, Nan, 30]. Mean=20.\n",
        "# 1. Transform('mean'): [20, 20, 20]\n",
        "# 2. Fillna: Nan replaced by 20.\n",
        "# Result: [10, 20, 30]\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'group': ['A', 'A', 'A', 'B', 'B', 'B'],\n",
        "    'value': [10.0, None, 30.0, 40.0, None, 60.0]\n",
        "})\n",
        "\n",
        "print(\"Original:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nImpute with mean:\")\n",
        "print(impute_missing(df, column='value', method='mean'))\n",
        "\n",
        "print(\"\\nImpute with group mean:\")\n",
        "print(impute_missing(df, column='value', method='group_mean', groupby='group'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 43: Missing Data Dropper ðŸŸ¡\n",
        "\n",
        "**Topic**: Removing rows/columns with missing values\n",
        "\n",
        "**Description**:\n",
        "Implement smart dropping strategies:\n",
        "- Drop rows with any missing values\n",
        "- Drop rows with all missing values\n",
        "- Drop rows with > threshold% missing\n",
        "- Drop columns with > threshold% missing\n",
        "- Keep only rows with at least N non-null values\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, None, None, 4],\n",
        "    'B': [None, None, None, None],\n",
        "    'C': [1, 2, 3, 4],\n",
        "    'D': [1, 2, None, 4]\n",
        "})\n",
        "\n",
        "drop_missing(df, axis=0, how='any', thresh=None)\n",
        "# Drops rows [0, 1, 2] (have any missing)\n",
        "\n",
        "drop_missing(df, axis=1, how='all')\n",
        "# Drops column B (all missing)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            "     A     B  C    D\n",
            "0  1.0  None  1  1.0\n",
            "1  NaN  None  2  2.0\n",
            "2  NaN  None  3  NaN\n",
            "3  4.0  None  4  4.0\n",
            "\n",
            "Drop rows with any missing:\n",
            "Empty DataFrame\n",
            "Columns: [A, B, C, D]\n",
            "Index: []\n",
            "\n",
            "Drop columns with all missing:\n",
            "     A  C    D\n",
            "0  1.0  1  1.0\n",
            "1  NaN  2  2.0\n",
            "2  NaN  3  NaN\n",
            "3  4.0  4  4.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def drop_missing(df, axis=0, how='any', thresh=None, subset=None):\n",
        "    \"\"\"\n",
        "    Wrapper for dropna with common parameters.\n",
        "    \n",
        "    Args:\n",
        "        axis: 0 (rows) or 1 (cols)\n",
        "        how: 'any' (drop if any NA) or 'all' (drop if all NA)\n",
        "        thresh: Keep only if at least N non-NA values\n",
        "        subset: Columns/Indices to consider\n",
        "    \"\"\"\n",
        "    # Pandas requires either 'how' OR 'thresh', not both (even if one is None in some versions)\n",
        "    if thresh is not None:\n",
        "        return df.dropna(axis=axis, thresh=thresh, subset=subset)\n",
        "    else:\n",
        "        return df.dropna(axis=axis, how=how, subset=subset)\n",
        "# Dry Run:\n",
        "# Input: DF with row of all Nans. axis=0, how='all'.\n",
        "# Result: Row dropped.\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, None, None, 4],\n",
        "    'B': [None, None, None, None],\n",
        "    'C': [1, 2, 3, 4],\n",
        "    'D': [1, 2, None, 4]\n",
        "})\n",
        "\n",
        "print(\"Original:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nDrop rows with any missing:\")\n",
        "print(drop_missing(df, axis=0, how='any'))\n",
        "\n",
        "print(\"\\nDrop columns with all missing:\")\n",
        "print(drop_missing(df, axis=1, how='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section J: Merge & Join (4 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 44: Basic Merge Operations ðŸŸ¢\n",
        "\n",
        "**Topic**: pd.merge() with different join types\n",
        "\n",
        "**Description**:\n",
        "Implement all four join types:\n",
        "- Inner join (intersection)\n",
        "- Left join (all from left)\n",
        "- Right join (all from right)\n",
        "- Outer join (union)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df1 = pd.DataFrame({\n",
        "    'key': ['A', 'B', 'C'],\n",
        "    'value1': [1, 2, 3]\n",
        "})\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'key': ['B', 'C', 'D'],\n",
        "    'value2': [4, 5, 6]\n",
        "})\n",
        "\n",
        "merge_dataframes(df1, df2, on='key', how='inner')\n",
        "# Returns:\n",
        "#   key  value1  value2\n",
        "# 0   B       2       4\n",
        "# 1   C       3       5\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inner join:\n",
            "  key  value1  value2\n",
            "0   B       2       4\n",
            "1   C       3       5\n",
            "\n",
            "Left join:\n",
            "  key  value1  value2\n",
            "0   A       1     NaN\n",
            "1   B       2     4.0\n",
            "2   C       3     5.0\n",
            "\n",
            "Outer join:\n",
            "  key  value1  value2\n",
            "0   A     1.0     NaN\n",
            "1   B     2.0     4.0\n",
            "2   C     3.0     5.0\n",
            "3   D     NaN     6.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def merge_dataframes(df1, df2, on=None, how='inner', left_on=None, right_on=None):\n",
        "    \"\"\"\n",
        "    Standard SQL-style join of two dataframes.\n",
        "    \"\"\"\n",
        "    return pd.merge(df1, df2, on=on, how=how, left_on=left_on, right_on=right_on)\n",
        "\n",
        "# Dry Run:\n",
        "# DF1 key=[A,B], DF2 key=[B,C]\n",
        "# Inner join on key:\n",
        "# Matches B. Result has B.\n",
        "\n",
        "# Test your solution\n",
        "df1 = pd.DataFrame({\n",
        "    'key': ['A', 'B', 'C'],\n",
        "    'value1': [1, 2, 3]\n",
        "})\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'key': ['B', 'C', 'D'],\n",
        "    'value2': [4, 5, 6]\n",
        "})\n",
        "\n",
        "print(\"Inner join:\")\n",
        "print(merge_dataframes(df1, df2, on='key', how='inner'))\n",
        "\n",
        "print(\"\\nLeft join:\")\n",
        "print(merge_dataframes(df1, df2, on='key', how='left'))\n",
        "\n",
        "print(\"\\nOuter join:\")\n",
        "print(merge_dataframes(df1, df2, on='key', how='outer'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 45: Concat and Join ðŸŸ¡\n",
        "\n",
        "**Topic**: pd.concat() and DataFrame.join()\n",
        "\n",
        "**Description**:\n",
        "Concatenate DataFrames vertically and horizontally:\n",
        "- Vertical concatenation (stacking rows)\n",
        "- Horizontal concatenation (adding columns)\n",
        "- Join on index\n",
        "- Handle mismatched indices\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
        "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
        "\n",
        "concat_vertical(df1, df2)\n",
        "# Returns:\n",
        "#    A  B\n",
        "# 0  1  3\n",
        "# 1  2  4\n",
        "# 0  5  7\n",
        "# 1  6  8\n",
        "\n",
        "concat_horizontal(df1, df2)\n",
        "# Returns:\n",
        "#    A  B  A  B\n",
        "# 0  1  3  5  7\n",
        "# 1  2  4  6  8\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertical concatenation:\n",
            "   A  B\n",
            "0  1  3\n",
            "1  2  4\n",
            "0  5  7\n",
            "1  6  8\n",
            "\n",
            "Horizontal concatenation:\n",
            "   A  B  A  B\n",
            "0  1  3  5  7\n",
            "1  2  4  6  8\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def concat_dataframes(df1, df2, axis=0, ignore_index=False):\n",
        "    \"\"\"\n",
        "    Concatenates dataframes vertically or horizontally.\n",
        "    \"\"\"\n",
        "    return pd.concat([df1, df2], axis=axis, ignore_index=ignore_index)\n",
        "\n",
        "# Dry Run:\n",
        "# Input: DF1 (2 rows), DF2 (2 rows). axis=0.\n",
        "# Result: stacked DF (4 rows), indices preserved unless ignore_index=True\n",
        "\n",
        "# Test your solution\n",
        "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
        "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
        "\n",
        "print(\"Vertical concatenation:\")\n",
        "print(concat_dataframes(df1, df2, axis=0))\n",
        "\n",
        "print(\"\\nHorizontal concatenation:\")\n",
        "print(concat_dataframes(df1, df2, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 46: Merge with Indicators ðŸŸ¡\n",
        "\n",
        "**Topic**: Advanced merge operations\n",
        "\n",
        "**Description**:\n",
        "Merge with indicator column to track source:\n",
        "- Add _merge column\n",
        "- Identify which rows came from left_only, right_only, or both\n",
        "- Handle suffixes for overlapping columns\n",
        "- Validate merge results\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df1 = pd.DataFrame({\n",
        "    'id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David']\n",
        "})\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'id': [2, 3, 4, 5],\n",
        "    'score': [85, 90, 78, 92]\n",
        "})\n",
        "\n",
        "merge_with_indicator(df1, df2, on='id')\n",
        "# Returns DataFrame with _merge column showing:\n",
        "# 'left_only', 'right_only', or 'both'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id     name  score      _merge\n",
            "0   1    Alice    NaN   left_only\n",
            "1   2      Bob   85.0        both\n",
            "2   3  Charlie   90.0        both\n",
            "3   4    David   78.0        both\n",
            "4   5      NaN   92.0  right_only\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def merge_with_indicator(df1, df2, on, how='outer'):\n",
        "    \"\"\"\n",
        "    Performs merge and adds '_merge' column indicating source.\n",
        "    Values: 'left_only', 'right_only', 'both'.\n",
        "    \"\"\"\n",
        "    return pd.merge(df1, df2, on=on, how=how, indicator=True)\n",
        "\n",
        "# Dry Run:\n",
        "# Merge DF1 (ids 1,2), DF2 (ids 2,3), outer.\n",
        "# id 1: left_only\n",
        "# id 2: both\n",
        "# id 3: right_only\n",
        "\n",
        "# Test your solution\n",
        "df1 = pd.DataFrame({\n",
        "    'id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David']\n",
        "})\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'id': [2, 3, 4, 5],\n",
        "    'score': [85, 90, 78, 92]\n",
        "})\n",
        "\n",
        "result = merge_with_indicator(df1, df2, on='id')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 47: Multi-Key Merge ðŸ”´\n",
        "\n",
        "**Topic**: Merging on multiple columns\n",
        "\n",
        "**Description**:\n",
        "Merge DataFrames on multiple keys:\n",
        "- Merge on 2+ columns\n",
        "- Handle composite keys\n",
        "- Different key names in each DataFrame\n",
        "- Validate merge integrity\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "sales = pd.DataFrame({\n",
        "    'year': [2023, 2023, 2024, 2024],\n",
        "    'quarter': ['Q1', 'Q2', 'Q1', 'Q2'],\n",
        "    'product': ['A', 'A', 'A', 'A'],\n",
        "    'revenue': [100, 150, 120, 180]\n",
        "})\n",
        "\n",
        "targets = pd.DataFrame({\n",
        "    'year': [2023, 2023, 2024, 2024],\n",
        "    'quarter': ['Q1', 'Q2', 'Q1', 'Q2'],\n",
        "    'target': [90, 140, 110, 170]\n",
        "})\n",
        "\n",
        "merge_multi_key(sales, targets, on=['year', 'quarter', 'product'])\n",
        "# Merges on all three keys\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year quarter product  revenue  target\n",
            "0  2023      Q1       A      100      90\n",
            "1  2023      Q2       A      150     140\n",
            "2  2024      Q1       A      120     110\n",
            "3  2024      Q2       A      180     170\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def merge_multi_key(df1, df2, on=None, how='inner', validate=None):\n",
        "    \"\"\"\n",
        "    Merge on multiple columns and validate relationship.\n",
        "    validate: 'one_to_one', 'one_to_many', etc.\n",
        "    \"\"\"\n",
        "    return pd.merge(df1, df2, on=on, how=how, validate=validate)\n",
        "\n",
        "# Dry Run:\n",
        "# Merge on [Year, Qtr]. \n",
        "# Matches rows where both Year AND Qtr are identical.\n",
        "\n",
        "# Test your solution\n",
        "sales = pd.DataFrame({\n",
        "    'year': [2023, 2023, 2024, 2024],\n",
        "    'quarter': ['Q1', 'Q2', 'Q1', 'Q2'],\n",
        "    'product': ['A', 'A', 'A', 'A'],\n",
        "    'revenue': [100, 150, 120, 180]\n",
        "})\n",
        "\n",
        "targets = pd.DataFrame({\n",
        "    'year': [2023, 2023, 2024, 2024],\n",
        "    'quarter': ['Q1', 'Q2', 'Q1', 'Q2'],\n",
        "    'target': [90, 140, 110, 170]\n",
        "})\n",
        "\n",
        "result = merge_multi_key(sales, targets, on=['year', 'quarter'])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section K: Advanced Transformations (3 Problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 48: String Operations ðŸŸ¡\n",
        "\n",
        "**Topic**: String methods in pandas\n",
        "\n",
        "**Description**:\n",
        "Perform various string operations on DataFrame columns:\n",
        "- Extract patterns (regex)\n",
        "- Split strings into multiple columns\n",
        "- Clean whitespace\n",
        "- Change case\n",
        "- Replace patterns\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@gmail.com'],\n",
        "    'phone': ['123-456-7890', '234-567-8901', '345-678-9012']\n",
        "})\n",
        "\n",
        "string_operations(df, column='email', operation='extract_domain')\n",
        "# Returns: ['gmail.com', 'yahoo.com', 'gmail.com']\n",
        "\n",
        "string_operations(df, column='phone', operation='remove_hyphens')\n",
        "# Returns: ['1234567890', '2345678901', '3456789012']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extract domain from email:\n",
            "0    gmail.com\n",
            "1    yahoo.com\n",
            "2    gmail.com\n",
            "Name: email, dtype: object\n",
            "\n",
            "Remove hyphens from phone:\n",
            "0    1234567890\n",
            "1    2345678901\n",
            "2    3456789012\n",
            "Name: phone, dtype: str\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def string_operations(df, column, operation, pattern=None):\n",
        "    \"\"\"\n",
        "    Applies string accessor methods to Series.\n",
        "    \"\"\"\n",
        "    if operation == 'extract_domain':\n",
        "        # Split by @ and take second part (index 1)\n",
        "        return df[column].str.split('@').str[1]\n",
        "    elif operation == 'remove_hyphens':\n",
        "        # Replace - with empty string\n",
        "        return df[column].str.replace('-', '')\n",
        "    return df[column]\n",
        "\n",
        "# Dry Run:\n",
        "# Input: 'a@b.com'. Op: extract_domain.\n",
        "# 1. Split -> ['a', 'b.com']\n",
        "# 2. Get [1] -> 'b.com'\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@gmail.com'],\n",
        "    'phone': ['123-456-7890', '234-567-8901', '345-678-9012']\n",
        "})\n",
        "\n",
        "print(\"Extract domain from email:\")\n",
        "print(string_operations(df, column='email', operation='extract_domain'))\n",
        "\n",
        "print(\"\\nRemove hyphens from phone:\")\n",
        "print(string_operations(df, column='phone', operation='remove_hyphens'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 49: DateTime Operations ðŸ”´\n",
        "\n",
        "**Topic**: Working with dates and times\n",
        "\n",
        "**Description**:\n",
        "Perform datetime operations:\n",
        "- Parse strings to datetime\n",
        "- Extract components (year, month, day, hour)\n",
        "- Calculate date differences\n",
        "- Resample time series\n",
        "- Create date ranges\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'date': ['2024-01-01', '2024-01-15', '2024-02-01'],\n",
        "    'value': [100, 150, 200]\n",
        "})\n",
        "\n",
        "datetime_ops(df, column='date', operation='extract_month')\n",
        "# Returns: [1, 1, 2]\n",
        "\n",
        "datetime_ops(df, column='date', operation='days_since_first')\n",
        "# Returns: [0, 14, 31]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extract month:\n",
            "[1, 1, 2]\n",
            "\n",
            "Days since first date:\n",
            "[0, 14, 31]\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "\n",
        "def datetime_ops(df, column, operation):\n",
        "    \"\"\"\n",
        "    Applies datetime accessor methods.\n",
        "    \"\"\"\n",
        "    # First convert to datetime objects\n",
        "    dates = pd.to_datetime(df[column])\n",
        "    \n",
        "    if operation == 'extract_month':\n",
        "        # Return month number (1-12)\n",
        "        return dates.dt.month.tolist()\n",
        "        \n",
        "    elif operation == 'days_since_first':\n",
        "        # Subtract minimum date from all dates to get timedelta\n",
        "        # Then extract 'days' component\n",
        "        return (dates - dates.min()).dt.days.tolist()\n",
        "        \n",
        "    return dates\n",
        "\n",
        "# Dry Run:\n",
        "# Input: [2024-01-01, 2024-01-10]\n",
        "# 1. min = 2024-01-01\n",
        "# 2. diffs = [0 days, 9 days]\n",
        "# 3. days -> [0, 9]\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'date': ['2024-01-01', '2024-01-15', '2024-02-01'],\n",
        "    'value': [100, 150, 200]\n",
        "})\n",
        "\n",
        "print(\"Extract month:\")\n",
        "print(datetime_ops(df, column='date', operation='extract_month'))\n",
        "\n",
        "print(\"\\nDays since first date:\")\n",
        "print(datetime_ops(df, column='date', operation='days_since_first'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 50: ML Feature Engineering Pipeline ðŸ”´\n",
        "\n",
        "**Topic**: Complete data preprocessing pipeline\n",
        "\n",
        "**Description**:\n",
        "Create a comprehensive feature engineering pipeline:\n",
        "1. Handle missing values\n",
        "2. Encode categorical variables\n",
        "3. Scale numerical features\n",
        "4. Create interaction features\n",
        "5. Remove outliers\n",
        "6. Split into train/test sets\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, None, 40, 100],  # Has missing and outlier\n",
        "    'income': [50000, 60000, 70000, 80000, 90000],\n",
        "    'category': ['A', 'B', 'A', 'C', 'B'],  # Categorical\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "pipeline = FeatureEngineeringPipeline()\n",
        "X_train, X_test, y_train, y_test = pipeline.fit_transform(\n",
        "    df, \n",
        "    target_col='target',\n",
        "    test_size=0.2\n",
        ")\n",
        "\n",
        "# Returns:\n",
        "# - Cleaned data\n",
        "# - Encoded categories\n",
        "# - Scaled features\n",
        "# - Train/test split\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (8, 3)\n",
            "X_test shape: (2, 3)\n",
            "y_train shape: (8,)\n",
            "y_test shape: (2,)\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class FeatureEngineeringPipeline:\n",
        "    \"\"\"\n",
        "    Complete pipeline: Missing -> Encode -> Outliers -> Scale -> Split.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Store parameters learned during fit\n",
        "        self.encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.means = {}\n",
        "    \n",
        "    def fit_transform(self, df, target_col, test_size=0.2):\n",
        "        data = df.copy()\n",
        "        \n",
        "        # 1. Handle missing values (Numerical mean imputation)\n",
        "        for col in data.select_dtypes(np.number):\n",
        "            if data[col].isnull().any():\n",
        "                self.means[col] = data[col].mean()\n",
        "                data[col] = data[col].fillna(self.means[col])\n",
        "                \n",
        "        # 2. Encode categorical variables (Label Encoding)\n",
        "        # Identify object/category columns excluding target\n",
        "        # Explicitly include 'string' to avoid Pandas4Warning/FutureWarning\n",
        "        try:\n",
        "            # Try including 'string' type (Pandas 1.0+)\n",
        "            cat_cols = data.select_dtypes(include=['object', 'category', 'string']).columns\n",
        "        except TypeError:\n",
        "            # Fallback for older pandas versions\n",
        "            cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
        "            \n",
        "        for col in cat_cols:\n",
        "            if col != target_col:\n",
        "                # Convert to category type then to codes\n",
        "                data[col] = data[col].astype('category')\n",
        "                # Store mapping for reproducibility (omitted here for brevity but important in prod)\n",
        "                self.encoders[col] = dict(enumerate(data[col].cat.categories))\n",
        "                data[col] = data[col].cat.codes\n",
        "                \n",
        "        # 3. Remove/Handle outliers (IQR Clipping)\n",
        "        # Apply to numeric features only (exclude target)\n",
        "        numeric_cols = data.select_dtypes(np.number).columns.drop(target_col, errors='ignore')\n",
        "        for col in numeric_cols:\n",
        "            q1 = data[col].quantile(0.25)\n",
        "            q3 = data[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 - 1.5 * iqr\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "            \n",
        "            # Clip values to bounds (keeps data size same, just caps extremes)\n",
        "            data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "            \n",
        "        # 4. Scale numerical features (MinMax Scaling)\n",
        "        # Formula: (x - min) / (max - min)\n",
        "        for col in numeric_cols:\n",
        "            min_val = data[col].min()\n",
        "            max_val = data[col].max()\n",
        "            self.scalers[col] = (min_val, max_val)\n",
        "            \n",
        "            if max_val != min_val:\n",
        "                data[col] = (data[col] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                data[col] = 0.0\n",
        "        \n",
        "        # 5. Split Data (Sequential split as per typical simple implementation)\n",
        "        # For random split, we would use np.random.permutation\n",
        "        split_idx = int(len(data) * (1 - test_size))\n",
        "        \n",
        "        # Use iloc for position-based slicing\n",
        "        train = data.iloc[:split_idx]\n",
        "        test = data.iloc[split_idx:]\n",
        "        \n",
        "        # Separate features (X) and target (y)\n",
        "        X_train = train.drop(columns=[target_col])\n",
        "        y_train = train[target_col]\n",
        "        X_test = test.drop(columns=[target_col])\n",
        "        y_test = test[target_col]\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test\n",
        "# Dry Run:\n",
        "# Input: DF with Age (missing), City (cat), Income (outlier).\n",
        "# 1. Age: Fill NaN with mean.\n",
        "# 2. City: 'A'->0, 'B'->1.\n",
        "# 3. Income: Clip values > Q3+1.5IQR.\n",
        "# 4. Scale: Map all numeric to [0, 1].\n",
        "# 5. Split: 80% Train, 20% Test.\n",
        "\n",
        "# Test your solution\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
        "    'income': [50000, 60000, 70000, 80000, 90000, 55000, 65000, 75000, 85000, 95000],\n",
        "    'category': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'C'],\n",
        "    'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "pipeline = FeatureEngineeringPipeline()\n",
        "X_train, X_test, y_train, y_test = pipeline.fit_transform(df, target_col='target', test_size=0.2)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai_ecomm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
